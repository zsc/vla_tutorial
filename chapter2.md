# 第2章 视觉模态：从像素到可行动的表征

## 开篇段落

视觉是具身智能体（Embodied Agent）感知和理解物理世界最主要、信息带宽最宽的通道。对于任何旨在“做对”而非仅仅“看懂”的VLA系统而言，视觉感知的失败是所有后续错误的根源，其代价可能是物理世界的真实损失。本章的目标，正是为VLA模型构建一个坚实可靠且“知其边界”的视觉感知基座。我们将沿着三条相互交织的技术主线，深入剖析视觉表征的演进：从奠定基石的卷积神经网络（CNN）及其分层、分部件的经典思想，到利用互联网规模图文数据进行大规模对比学习、打通视觉与语言鸿沟的对齐范式（如CLIP），再到挖掘视频数据内蕴时空结构与物理规律的自监督学习方法（如MAE）。本章不仅回顾这些方法的原理，更将系统性地梳理现代VLM如何基于这些思想构建出不同的主流架构。最终，所有讨论将收敛于两个对于构建可部署行动系统至关重要的议题：**开集识别与不确定性估计**，以及**时域错位鲁棒性**。这确保我们的感知基座不仅看得准，更能在面对未知时“知其不知”，并能优雅地处理真实世界中无处不在的延迟与异步。

---

## 文字论述

### 2.1 经典视觉回顾：CNN 分层特征与“部件—整体”分解

在深度习革命之前，计算机视觉依赖于手工设计的特征提取器（如SIFT, HOG）。卷积神经网络（CNN）的崛起，标志着从“特征工程”到“特征学习”的范式转移。其成功源于对自然图像统计特性的深刻洞察和三个核心设计原则：

1.  **局部感受野 (Local Receptive Fields)**：每个神经元只连接到输入层的一个小区域，这模拟了生物视觉皮层的工作方式，并利用了图像中像素相关性的局部性原理。
2.  **权值共享 (Weight Sharing)**：一个特征检测器（卷积核）在整个图像上滑动，共享同一套权重。这极大地减少了模型参数，并使其具备了**平移不变性 (Translation Invariance)**。
3.  **层次化特征提取 (Hierarchical Feature Extraction)**：通过堆叠卷积层和池化层，网络能够构建出从简单到复杂的特征层次。底层学习边缘、角点等基础纹理；中层组合成物体的部件（如车轮、眼睛）；高层则汇聚成完整的物体或场景概念。

*   **代表性工作：ResNet (Residual Network, He et al., 2015)**
    *   **贡献**：通过引入“残差连接”（Shortcut Connection），革命性地解决了深度网络训练的难题，使得训练数百甚至上千层的超深网络成为现实，并成为后续无数视觉模型的基础骨干。
    *   **核心思想**：与其让一个网络层（或块）直接学习一个复杂的目标映射 $H(x)$，不如让它学习该映射与输入 $x$ 之间的残差 $F(x) = H(x) - x$。原始映射则通过一个恒等短路连接变为 $H(x) = F(x) + x$。
    $$
    y = \mathcal{F}(x, \{W_i\}) + x
    $$
    这个简单的加法操作，为梯度在网络中的反向传播提供了一条“高速公路”，确保了信息的无损流动。

### 2.2 Marr 式表征思想与多尺度处理

认知科学家大卫·马尔（David Marr）提出的视觉计算理论框架强调从多尺度信息中逐步恢复世界结构的重要性。这个思想在现代深度学习中具体化为对**多尺度特征**的精妙处理。

*   **代表性工作：FPN (Feature Pyramid Network, Lin et al., 2017)**
    *   **贡献**：提出了一种简洁而高效的架构，用于在所有尺度上构建具有丰富语义信息的特征金字塔，极大地提升了目标检测和分割等任务的性能，已成为事实上的标准组件。
    *   **结构**：FPN通过自顶向下的路径和横向连接，将高层（语义丰富但分辨率低）特征图的信息逐步融合到低层（细节精确但语义弱）特征图中。

```ascii
      C5 (高语义) ---> [1x1 Conv] -------------------> P5 (融合后)
         |               |                              |
         v (自底向上)      +--> [Upsample x2] --(+)--> [3x3 Conv] -> P4
         |               |                     ^
      C4 ------------> [1x1 Conv] --------------+
      ...             ...                     ...
```

**经验法则 (Rule-of-thumb):** 在处理需要同时关注全局与局部感知的机人或自动驾驶任务时，使用FPN或其变体结构（如PANet, BiFPN）几乎是必须的。

### 2.3 视觉—语言对齐与现代VLM架构范式

CLIP的成功不仅验证了大规模图文对比学习的有效性，更催生了现代VLM的主流架构范式。核心问题变为：如何最高效、最有力地将强大的预训练视觉编码器（如ViT）与预训练语言模型（LLMs）结合起来？不同的回答形成了当今VLM世界的三大主流设计哲学。

#### 2.3.1 范式一：面向巨人的适配器——以Flamingo为代表
这是“站在巨人肩膀上”的典范，其核心思想是**最大化地复用并保持强大的预训练视觉编码器和LLM基本冻结**，仅通过一个轻量级的、可训练的“适配器”模块来高效地桥接两者，以弥合模态鸿沟（modality gap）。

*   **代表性工作：Flamingo (Alayrac et al., 2022)**
    *   **设计哲学**：在计算资源有限或希望快速验证想法时，完全从零训练或全量微调巨大的基座模型是不切实际的。Flamingo假设视觉和语言的通用知识已分别被其基座模型充分学习，唯一需要学习的是如何将视觉概念“翻译”成LLM能够理解的“语言”。
    *   **架构创新**：
        1.  **Perceiver Resampler**：这是一个精巧的瓶颈模块。视觉编码器（如ViT）对图像会产生上百个特征向量（tokens），而将这么多向量直接注入LLM会带来巨大的计算负担。Perceiver Resampler通过少量可学习的“查询”向量，利用交叉注意力机制将可变数量的视觉特征“蒸馏”成固定数量（例如64个）的、低维的“潜”向量。这本质上是一种可学习的视觉信息压缩。
        2.  **Gated Cross-Attention Layers**：这些新引入的、可训练的交叉注意力层被**交错地（interleaved）**插入到**冻结的**LLM的原始自注意力层之间。在每个这样的层，语言token可以attend到由Perceiver Resampler生成的视觉潜向量。关键在“Gated”（门控）机制，它通过一个学习到的标量来控制该层交叉注意力的输出应在多大程度上影响LLM的中间表示，让模型自己决定在推理的不同阶段“看多少眼”图像。
    *   **优势与影响**：由于只训练极少量的参数（适配器部分），Flamingo的训练效率极高。它展示了惊人的少样本（few-shot）上下文学习能力，能够通过几个例子快速适应新的图文任务，为参数高效的VLM设计树立了标杆。

#### 2.3.2 范式二：端到端与多阶段联合训练——以PaLI/PaliGemma和InternVL为代表
与冻结组件不同，该范式主张对视觉和语言模块进行更深度的联合训练，以期获得更强的性能上限。其哲学是，模态间的深度融合需要让两个模型的参数都能相互适应，而不仅仅是单向的“翻译”。这通常采用复杂而精心设计的多阶段训练策略。

*   **代表性工作：PaLI/PaliGemma/PaliGemma2 (Chen et al., 2022; Beyer et al., 2024)**
    *   **设计哲学**：模型性能的极限最终取决于数据和模型参数的协同优化。通过精心设计的训练日程，可以在不同阶段侧重于学习不同层面的能力（通用语义、空间关系、高分辨率细节等），从而在最终的下游任务上获得最佳的迁移性能。
    *   **训练策略**：`PaliGemma`论文中描述了一个典型的多阶段流程，这已成为开源社区的主流实践：
        *   **Stage 0 (单模态预训练)**：直接复用在各自领域（如`PaliGemma`使用**SigLIP**和**Gemma**）预训练好的视觉和语言模型作为初始化。
        *   **Stage 1 (多模态预训练)**：在一个由图像描述、视觉问答、OCR、物体检测等任务**精心混合**的大规模图文数据上联合训练整个模型。与Flamingo的关键不同在于，**此阶段通常会解冻视觉编码器**（或至少部分解冻）。其动机在于，CLIP/SigLIP这类对比学习模型擅长全局语义，但在需要精确定位的任务上可能并非最优。允许视觉编码器在更丰富的监督信号（如来自检测和分割任务的文本）下进行微调，可以使其学习到对下游任务更有价值的空间和关系理解能力。
        *   **Stage 2 (分辨率提升)**：在更高分辨率的图像上进行短时间的持续预训练，让模型适应更精细的视觉细节。
        *   **Stage 3 (迁移微调)**：在具体的下游任务（如自动驾驶场景理解）上进行最终的微调。
*   **代表性工作：InternVL (Chen et al., 2023) 和 Qwen-VL (Bai et al., 2023)**：这两个模型是该范式在“规模”上的极致体现。它们通过在海量（百亿甚至千亿级token）、高质量、经严格清洗和配比的多模态数据上进行持续的多阶段预训练，将开源VLM的性能推向了新的高度，展现了端到端联合训练在深度融合多模态信息上的巨大潜力。

#### 2.3.3 范式三：万物皆为序列的统一——以OFA和Emu3为代
这是最具野心和理论优雅性的范式，其目标是将**所有模态（图像、视频、文本、语音等）都编码为统一的离散token序列**，然后用一个单一的、巨大的自回归Transformer模型，通过标准的“下一个token预测”（Next-Token Prediction）目标来进行统一建模。

*   **代表性工作：OFA (One-For-All, Wang et al., 2022)**
    *   **设计哲学**：任务的边界是人为的。无论是图像描述（输入图，输出文）、视觉问答（输入图+文，输出文）还是物体检测（输入图+文，输出坐标），都可以统一为一种“序列到序列”的生成任务。
    *   **架构创新**：OFA通过一个共享的词表来表示不同信息：文本用BPE分词，图像区域通过将其坐标（x, y, w, h）**量化**成离散的token来表示，图像内容则通过一个预训练的VQ-VAE编码器压缩成离散的视觉token。然后，一个标准的Transformer编码器-解码器模型被训练来处理这些混合了不同类型token的序列。
*   **代表性工作：Emu/Emu2/Emu3 (BAAI, 2023/2024)**
    *   **设计哲学**：“Next-Token Prediction is All You Need”。如果说OFA还依赖于预训练的VQ-VAE，Emu3则将这一理念推向极致。它致力于构建一个真正端到端的、仅依赖单一目标的系统。
    *   **架构创新**：Emu3的核心是一个强大的**视觉分词器（Vision Tokenizer）**。这是一个独立的、经过精心训练的编码器-解码器模型，其唯一任务就是将图像甚至视频帧无损地“压缩”成离散的token序列，并能从这些token中完美重建回原始像素。一旦有了这个分词器，整个VLM的训练就变得异常简单：将文本token序列和视觉token序列拼接起来，然后用一个巨大的、从零开始训练的纯解码器Transformer模型（类似GPT）去预测下一个token。
    *   **优势与影响**：架构极其简洁统一，摆脱了对CLIP、扩散模型等其他预训练组件的依赖。它然地统一了**感知**（通过编码序列来理解）和**生成**（通过自回归地预测视觉token来创造图像和视频），展示了用语言模型的基本原理统一多模态AI的巨大潜力。这是通往通用多模态智能的一条非常有前景、但也对算力和数据要求极高的路径。

### 2.4 超越固定尺寸：处理高分辨率与可变尺寸输入
早期VLM的一个普遍限制是它们只能处理固定尺寸、通常是正方形的低分辨率输入（如224x224），这对于需要精细细节的任务（如OCR、细粒度识别）是致命的。近期工作开始致力于打破这一限制，这对具身智能尤为重要。

*   **代表性工作：Kimi-VL (Moonshot AI, 2025) 与 Seed-1.5-VL (ByteDance, 2025)**
    *   **核心思想**：采用**原生分辨率视觉编码器 (Native-resolution Vision Encoder)**。其灵感源于NaViT，核心在于放弃将不同尺寸的图像强制缩放到固定大小。取而代之的是，无论图像原始尺寸和长比如何，都将其统一划分为固定大小的patch（例如14x14）。
    *   **技术实现**：
        1.  **打包（Packing）**：不同图像会产生数量不同的patch。这些patch序列被“打包”成一个更长的、连续的1D序列输入到Transformer中，不同图像的patch之间用特殊分隔符隔开，并通过注意力掩码（attention mask）确保它们在计算中相互隔离。
        2.  **2D RoPE（旋转位置编码）**：由于patch序列的长度可变，传统的学习式1D位置编码不再适用。2D RoPE被用来对每个patch在原始2D图像中的`（x, y）`坐标进行编码，赋予模型灵活处理任意尺寸和长宽比输入的能力。
*   **代表性工作：MiniMax-VL (MiniMax, 2025)**
    *   **核心思想**：采用动态多尺度策略。它不依赖于单一的原生分辨率输入，而是将输入图像处理成多个不同分辨率的版本，包括一个标准的低分辨率缩略图和多个高分辨率的裁剪块。
    *   **技实现**：这些多尺度的图像块被独立编码后，其特征在送入LLM之前被融合。这种策略让模型能同时获得“全局概览”（来自低分辨率缩略图）和“局部细节”（来自高分辨率块），在需要跨尺度理解的文档问答和图表分析任务中表现出色。同时，其对`Lightning Attention`（一种线性注意力变体）的探索也旨在高效处理高分辨率图像产生的巨量token序列。

### 2.5 视频自/半监督：时空结构的挖掘
*   **代表性工作：VideoMAE (Tong et al., 2022)**
    *   **贡献**：将在NLP和图像领域大获成功的掩码自编码器思想成功迁移到视频。它证明了视频信号具有极高的时空冗余，通过随机掩码掉极高比例的视频块（如90%）并让模型去重建，可以学到非常强大的时空表征。

### 2.6 开放问题与路线组合
**经验法则 (Rule-of-thumb):** 一个经过实战检验的、稳健的视觉基座构建策略通常遵循阶段化设计：
1.  **阶段一（获取通用语义先验）**：使用在海量图文对上预训练好的模型（如CLIP或SigLIP的ViT变体）作为视觉编码器的**初始化权重**。
2.  **阶段二（学习领域动态特性）**：在大量与目标任务相关的无标签视频数据上，使用掩码重建（如VideoMAE）进行第二阶段的预训练。
3.  **阶段三（任务导向的微调）**：在最终的目标任务上，将视觉模型与其他模态连接起来，进行端到端的微调。

### 2.7 开集识别、不确定性估计与时域错位鲁棒性
对于需要做出物理行动的VLA系统，处理未知和不确定性是保证安全的核心要求。

1.  **开集识别与不确定性估计 (Open-Set Recognition & Uncertainty Estimation)**
    *   **问题**：模型在部署时会遇到训练集中从未见过的物体或场景（分布外，Out-of-Distribution, OOD）。一个安全的系统不应将路上的一个纸箱强行识别为“石头”并紧急刹车，而应该报告“知障碍物，置信度低”。
    *   **方法：能量分数 (Energy Score)**：一个理论更优雅的方法。对于分类器的logits $f_i(x)$，其能量函数可定义为 $E(x; f) = -T \cdot \log \sum_i e^{f_i(x)/T}$。研究表明，分布内样本的能量分数显著低于OOD样本，因此可以设置一个阈值来高效地检测异常输入。

2.  **时域错位鲁棒性 (Temporal Misalignment Robustness)**
    *   **问题**：这是一个系统工程问题。从光子击中传感器到控制指令到达执行器，整个VLA系统中存在着多个环节的延迟。
    *   **缓解策略**：
        *   **精确时间戳与系统级同步**：这是基石。
        *   **状态估计与预测**：在感知和决策之间引入一个状态估计器（如卡尔曼滤波器）。该估计器融合多传感器历史数据，来预测出“当前时刻”的真实世界状态。

---

## 本章小结

本章我们系统地剖析了构建一个现代、强大的VLA视觉基座所涉及的心技术与思想演进。我们不仅回顾了经典视觉（CNN）、视觉-语言对齐（CLIP）和视频自监督（MAE）等奠基性思想，更深入地剖析了现代VLA模型的三大主流架构范式：

1.  **适配器范式 (如 Flamingo)**：冻结强大的基座模型，通过轻量级模块进行高效桥接，擅长少样本学习，是参数高效的典范。
2.  **联合训练范式 (如 PaliGemma, InternVL)**：通过端到端或多阶段的联合训练，实现模态间的深度融合，追求更高的性能上限，是当前开源社区的主流。
3.  **统一序列范式 (如 OFA, Emu3)**：将所有模态分词化，用单一的Next-Token Prediction目标统一所有任务，架构优雅，是通往通用多模态智能的潜力路径。

我们还探讨了处理**高分辨率和可变尺寸输入**的前沿方法（如Kimi-VL, Seed-1.5-VL），这对于提升VLA在精细任务上的能力至关重要。最后，一个面向行动的理想视觉基座，必须能够量化自身输入的不确定性（**能量分数、集成方法**），并能稳健地处理真实物理世界中无处不在的**时域错位**（**时间戳、状态预测**）。这些特性是后续章节中实现可靠对齐、安全RL和成功Sim-to-Real迁移的绝对前提。

---

## 常见陷阱与错误 (Gotchas)

1.  **陷阱：将 ImageNet 精度等同于真实世界鲁棒性。**
    *   **表现**：过度迷信在标准学术数据集上的SOTA排名，而忽略了模型在特定部署环境下的长尾场景表现。
    *   **调试技巧**：建立并维护一个专门针对“痛点”的评估集（"Pain-Point" Benchmark），包含大量来自真实部署环境的失败案例、边界条件和对抗性样本。

2.  **陷阱：混淆“基础模型”与“指令微调模型”的评测目标。**
    *   **表现**：使用一个为“迁移微调”（Transfer Fine-tuning）设计的“基础VLM”（如PaliGemma）去直接进行零样本的指令跟随评测，并得出其性能不佳的结论。
    *   **调试技巧**：明确模型的设计目标。基础模型（Base Model）的目标是学习通用、可迁移的表征，其评测应侧重于下游任务微调后的性能。指令微调模型（Instruction-tuned Model）的目标是遵循零样本或少样本指令，其评测应侧重于直接的问答和指令跟随能力。

3.  **陷阱：模型“自信地幻视”而未被系统拦截。**
    *   **表现**：一个VLM在描述图像时，生成了听起来非常合理但图像中完全不存在的物体或细节（即“幻觉”，Hallucination），且输出置信度很高。
    *   **调试技巧**：绝不单独依赖softmax输出作为置信度。部署流程中强制加入不确定性监控（如能量分数）。此外，可以引入“可追溯性”或“引用”机制，要求模型在生成描述时，指出其描述对应图像的哪个区域。

4.  **陷阱：忽视视觉输入的时域特性。**
    *   **表现**：模型对时间抖动、延迟或丢帧极其敏感，因为其没有学到物体的运动连续性和动态一致性。
    *   **调试技巧**：在仿真和回放测试中，系统性地注入时间扰动。检查所有传感器的同步日志，确保时间戳对齐是硬件和软件层面的第一优先级。
