# 第8章 强化学习与微调：从指令遵循到策略优化

## 8.1 开篇段落

在前面的章节中，我们已经构建了一个强大的 VLA 预训练基座模型，它能够通过模仿学习（如行为克隆）对世界产生初步的理解和行动能力。然而，仅仅“模仿”是不够的。真实世界的任务要求智能体能够**优化**性能、适应未见过的场景、并从交互的后果中学习，而不仅仅是复制专家数据。本章将深入探讨如何通过强化学习（RL）和相关微调技术，将我们的 VLA 模型从一个被动的“模仿者”转变为一个主动的“决策者”。我们将系统地比较监督微调（SFT）、基于反馈的微调（RFT）和经典强化学习的优劣与适用场景，并介绍策略优化的核心算法、自反式指导等前沿技术。特别地，我们将借鉴 **Seed1.5-Thinking** 等前沿工作中的经验，深入探讨在处理需要复杂推理（如长思维链 CoT）的任务时，如何设计数据、奖励模型和算法来克服 RL 训练的**不稳定性**。本章的学习目标是，掌握一套将预训练模型转化为能够进行目标导向优化、同时兼顾安全与效率的高级策略的完整方法论，并学会使用离线策略评估（OPE）在部署前科学地评估其潜在性能，为后续的仿真与 Sim-to-Real 迁移奠定坚实的策略基础。

---

## 8.2 文字论述

### 8.2.1 SFT vs. RFT vs. RL：记忆与泛化的权衡

从预训练基座模型出发，提升其特定任务能力的路径并非只有一条。选择哪条路径，取决于数据可用性、任务复杂度和对泛化能力的要。

*   **监督微调 (Supervised Fine-Tuning, SFT)**
    *   **核心思想**: SFT 本质上是**行为克隆 (Behavior Cloning, BC)** 的延续。它使用高质量的“状态-行动”对 `(s, a)` 数据集，通过最大化专家行动的对数似然来进行微调。
        $$
        L_{SFT}(\theta) = -\mathbb{E}_{(s, a) \sim \mathcal{D}_{expert}} [\log \pi_\theta(a|s)]
        $$
    *   **代表性工作**: 几乎所有基于模仿学习的机器人和自动驾驶模型，如早期的 **ALVINN** 到现代的 **Gato** 和 **RT-1**，都以 SFT/BC 作为其核心训练范式或初始阶段。**Seed1.5-Thinking** 等工作也明确指出，从一个经过 SFT 的模型开始 RL，相比直接从基座模型开始，能够产出更可读的输出、减少幻觉，为后续的 RL 阶段提供一个坚实的基础。
    *   **优点**: 简单、稳定、对数据质量要求高但利用直接。是冷启动一个策略的最佳起点。
    *   **缺点**:
        1.  **分布偏移 (Distribution Shift)**: 模型在训练分布上表现良好，但一旦在执行中遇到一个微小的、未见过的状态，其后续的轨迹可能会迅速偏离专家数据覆盖的范围，导致错误累积。
        2.  **因果混淆 (Causal Confusion)**: 模型可能学习到虚假的关联而非真正的因果关系。
        3.  **次优天花板**: 模型的性能上限被数据集中最好的专家所限制，无法发现超越专家的更优策略。

*   **基于反馈的微调 (Reinforcement Fine-Tuning, RFT)**
    *   **核心思想**: 此类方法，以 **RLHF (Reinforcement Learning from Human Feedback)** 和 **RLAIF (RL from AI Feedback)** 为代表，承认了为复杂任务设计一个精确、稠密的奖励函数极其困难。人类（或更强的 AI 模型）更擅长做**比较判断**。其流程通常为：
        1.  从当前策略中采样若干个轨迹或行动序列。
        2.  让人类或 AI 标注者对这些序列进行偏好排序。
        3.  利用这些偏好数据训一个**奖励模型 (Reward Model, RM)**，$r_\phi(s, a)$，使其能够预测人类的偏好。通常使用 Bradley-Terry 模型，其损失函数为：
            $$
            L(\phi) = -\mathbb{E}_{(s, a_w, a_l) \sim \mathcal{D}_{pref}} [\log(\sigma(r_\phi(s, a_w) - r_\phi(s, a_l)))]
            $$
            其中 $a_w$ 是更优的行动 (winner)，$a_l$ 是较差的行动 (loser)。
        4.  使用强化学习算法（如 PPO）优化策略 $\pi_\theta$，以最大化来自奖励模型 $r_\phi$ 的期望回报。
    *   **代表性工作**: **InstructGPT** 和 **ChatGPT** 在语言模型领域的成功，将 RLHF 推向了主流。在 VLA 领域，这被用于对齐如“驾驶舒适度”、“操作灵巧性”等主观指标。
    *   **优点**: 能够对齐模糊、主观的人类价值观，绕过了手动设计奖励函数的难题。
    *   **缺点**: 奖励模型本身可能存在偏差或被“黑客攻击”(Reward Hacking)，并且需要持续的人工或 AI 标注，成本高。

*   **环境交互强化学习 (RL from Environment Interaction)**
    *   **核心思想**: 智能体直接与环境（通常是仿真器）交互，接收环境反馈的标量奖励信号 $r(s, a)$，并以此优化策略以最大化累积回报 $G_t = \sum_{k=0}^\infty \gamma^k r_{t+k+1}$。
    *   **优点**: 潜力无限，能够发现超越人类的、全新的解决方案。
    *   **缺点**: **样本效率极低**，尤其是在真实世界中。奖励函数设计是关键难点，且在线探索可能带来安全风险。

> **Rule-of-Thumb**:
> 1.  **启动**: 永远从 SFT/BC 开始，获得一个可用的基础策略。
> 2.  **对齐**: 当任务目标难以用数值量化（如“优雅地”并线）时，转向 RFT/RLHF。
> 3.  **极限优化**: 当你拥有一个高保真仿真器和明确的性能指标（如通行时间、能耗）时，使用经典 RL 进行极限压榨。
> 4.  **混合使用**: 最佳实践通常是三者的结合：用 SFT 初始化，用 RLHF 对齐复偏好，同时用环境稀疏奖励（如任务成功）作为补充，并始终用 BC loss 进行正则，防止策略“忘记”基本技能。

### 8.2.2 策略优化：核心算法与正则化

*   **PPO (Proximal Policy Optimization)**: 这是当前 RL 领域的“瑞士军刀”。PPO 通过一个**截断的替代目标函数 (Clipped Surrogate Objective)** 来限制每次更新的步长。
    $$
    L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t) \right]
    $$
    其中，$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ 是重要性采样权重，$\hat{A}_t$ 是优势函数估计。

*   **离线强化学习 (Offline RL)**: 如何直接利用海量的离线数据集进行 RL 而无需昂贵的在线交互，是离线 RL 的核心问题。
    *   **代表性工作**:
        *   **CQL (Conservative Q-Learning)**: 在 Q 函数的学习目标上增加一个正则项，惩罚在数据分布外的 `(s, a)` 对的高 Q 值。
        *   **IQL (Implicit Q-Learning)**: 通过期望回归，隐式地学习一个 Q 函数，避免了对 OOD `(s, a)` 的显式查询。

### 8.2.3 稳定化RL：来自 Seed1.5-Thinking 的前沿实践

VLA 模型中的语言模块在执行复杂任务时，需要生成长的思维链（CoT）或行动计划。对此类长序列任务进行 RL 训练是**高度不稳定**的。**Seed1.5-Thinking** 等工作系统性地解决了这个问题，其核心思想可以归纳为一套组合拳，对我们极具借鉴意义：

1.  **价值函数预训练 (Value Pre-training)**: 在开始 PPO 训练前，固定 SFT 策略，采样轨迹并使用蒙特卡洛回报来预训练价值模型。这确保了初始的价值函数与初始策略高度对齐，为后续的优势估计提供一个稳定、准确的起点，是防止早期训练崩溃的关键一步。

2.  **精细化的优势估计 (Refined Advantage Estimation)**:
    *   **解耦 GAE (Decoupled-GAE)**: 为价值函更新和策略函数更新使用不同的 GAE 参数 $\lambda$（例如，$\lambda_{value}=1.0$ 做无偏更新，$\lambda_{policy}=0.95$ 来降低方差）。这使得价值和策略的优化目标可以解耦，更灵活地平衡各自的偏置-方差权衡。
    *   **长度自适应 GAE (Length-adaptive GAE)**: 将 $\lambda$ 设置为与响应长度 $l$ 相关的函数，如 $\lambda_{policy} = 1 - \frac{\alpha}{l}$。这使得长序列的 TD 误差分布更均匀，有效处理不同长度的 CoT。

3.  **改进的 PPO 裁剪 (Improved PPO Clipping)**:
    *   **非对称裁剪 (Clip-Higher)**: PPO 的标准对称裁剪可能过度抑制对低概率、但可能是正确探索方向的 token 的学习。通过使用非对称的裁剪边界，可以给予“好”的 token 更大的更新空间。
        $$
        L^{CLIP-H}(\theta) = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon_{low}, 1+\epsilon_{high})\hat{A}_t) \right]
        $$
        其中 $\epsilon_{high} > \epsilon_{low}$，这鼓励模型探索新的、有益的行动。

4.  **混合损失函数 (Hybrid Loss Function)**:
    *   **正样本语言模型损失 (Positive Example LM Loss)**: RL 训练容易导致语言流畅性下降（所谓的“语言漂移”）。一个有效的对抗方法是，在 PPO 损失之外，对那些被判定为“正样本”（例如，成功完成任务或获得高奖励）的轨迹，额外施加一个标准的监督学习损失（NLL Loss）。
        $$
        L(\theta) = L_{PPO}(\theta) + \mu \cdot L_{NLL}(\theta) \quad \text{for positive examples}
        $$
    这有助于模型在探索新策略的同时，不遗忘从 SFT 阶段学到的优秀语言模式和基本技能。

### 8.2.4 自反式指导与奖励设计

*   **自反式指导**: 大型 VLA 模型，特别是融合了强大语言能力的模型，具备了进行“自我反思”和“自我提升”的潜力。模型执行一个任务后，可以根据结果（成功、失败、或人类反）进行自我反思，生成“失败-原因-修正”的文本，并将其作为新的训练数据，形成一个持续改进的闭环。

*   **奖励设计：可验证 vs. 不可验证**:
    *   **可验证问题 (Verifiable Problems)**: 对于有明确正确答案的任务（如数学计算、代码生成、闭环控制中的目标达成），我们可以设计**自动化验证器 (Verifier)** 来提供准确、二元的奖励信号（成功/失败）。**Seed1.5-Thinking** 甚至更进一步，训练了一个 **“会思考”的验证器 (Thinking Verifier)**，它不仅给出判断，还生成判断的推理过程（CoT），这使得奖励信号更鲁棒，能有效对抗奖励黑客。在 VLA 中，这对应于“车辆是否在停车线内停稳”、“机械臂是否将物体插入指定孔位”。
    *   **不可验证问题 (Non-verifiable Problems)**: 对于那些依赖人类偏好的任务（如“以更‘拟人’的方式驾驶”、“生成一段对场景的‘生动’描述”，我们则依赖于上面提到的 RFT/RLHF 框架，即训练一个奖励模型来拟合人类的偏好。

### 8.2.5 OPE（离线策略评估）：不上线如何知好坏

在将新训练的策略 $\pi_e$ 部署到昂贵或危险的真实环境中之前，我们迫切需要一种方法，利用已有的离线数据集（由旧的或行为策略 $\pi_b$ 收集）来评估 $\pi_e$ 的性能。

*   **重要性采样 (Inverse Propensity Scoring, IPS)**: 通过重要性权重 $\frac{\pi_e(a|s)}{\pi_b(a|s)}$ 来修正回报，但方差极大。
*   **双重鲁棒估计 (Doubly Robust, DR)**: 结合了基于模型的估计（如 Q 函数）和 IPS，显著降低了方差，是当前最实用的 OPE 方法之一。
*   **拟合Q评估 (Fitted Q-Evaluation, FQE)**: 直接在离线数据集上，为待评估的策略 $\pi_e$ 学习一个 Q 函数 $Q^{\pi_e}$。

> **Rule-of-Thumb**:
> 在进行任何 RL 微调后，**必须**运行 OPE。首选 DR 估计器来获得策略价值的点估计和置信区间。**永远不要完全信任 OPE 的结果**，它只是部署决策的辅助依据，而非最终判决。

---

## 8.3 本章小结

本章系统地阐述了如何将预训练 VLA 模型通过微调和强化学习，从一个模仿者提升为高效的决策者。我们明确了 SFT、RFT 和 RL 的核心差异与适用场景，并强调了它们在实践中通常是互补而非互斥的。

*   **关键概念**:
    *   **微调谱系**: 从 SFT 到 RFT 再到 RL，是一个从纯粹模仿到价值对齐，再到自主优化的过程。
    *   **稳定化RL技术**: 对于需要长程推理的 VLA 任务，必须采用一套组合拳来稳定 RL 训练，包括**价值预训练、解耦/自适应 GAE、非对称 PPO 裁剪**和**混合损失函数**。
    *   **奖励工程**: 核心是区分**可验证**（使用自动化/思考式验证器）和**不可验证**（使用偏好奖励模型）的任务，并设计相应的奖励机制。
    *   **自反式指导**: 利用 LLM 的能力进行 CoT 评估和我纠正，是实现模型持续自我提升的有前途的方向。
    *   **离线评估 (OPE)**: IPS、DR 和 FQE 是在部署前评估策略性能的必要工具，为决策提供数据支持。
    *   **策略整形**: 平滑、延迟补偿和安全裕度是确保学习策略在真实世界中可用、可靠的最后一道工序。

---

## 8.4 常见陷阱与错误 (Gotchas)

1.  **奖励黑客 (Reward Hacking)**
    *   **现象**: 智能体找到了一个最大化奖励的“捷径”，但这个行为完全违背了设计者的初衷。
    *   **调试技巧**: 对于可验证任务，使用更强大的验证器（如“思考式验证器”）来减少漏洞。对于不可验证任务，定期更新奖励模型，并引入多样化的、对抗性的偏好数据。在训练过程中定期审查智能体的行为录像。

2.  **长程任务的RL训练崩溃**
    *   **现象**: 在需要长 CoT 或多步行动序列的任务中，PPO 训练过程非常不稳定，奖励曲线剧烈震荡，至完全不收敛。
    *   **调试技巧**: 不要期望 vanilla PPO 能直接解决问题。系统性地实施本章 8.2.3 节中介绍的稳定化技术组合：从价值预训练开始，确保优势估计的基线是可靠的；然后调整 GAE 和 PPO 裁剪策略；最后加入正样本 LM 损失来防止语言能力退化。这是一个系统工程，而非单个超参调整。

3.  **对 OPE 结果的盲目乐观**
    *   **现象**: OPE 报告显示新策略性能提升 20%，但上线后表现反而下降。
    *   **调试技巧**: 始终报告 OPE 估计的**置信区间**。检查行为策略 $\pi_b$ 和评估策略 $\pi_e$ 的重合度，如果差异过大，OPE 结果将不可信。将 OPE 视为一个“红绿灯系统”，而非精确的数值预测。

4.  **灾难性遗忘 (Catastrophic Forgetting)**
    *   **现象**: 在为任务 B 微调一个精通任务 A 的模型后，模型在任务 A 上的性能大幅下降。
    *   **调试技巧**: 在微调的数据中混入一部分原始任务 A 的数据。使用行为克隆正则化或**正样本 LM 损失**，确保新策略不会离原始策略太远。

5.  **忽略策略输出的物理可实现性**
    *   **现象**: RL 策略输出了一个理论上最优但在物理上无法执行的行动序列（例如，要求电机瞬间改变速度）。
    *   **调试技巧**: 在奖励函数中加入对行动的平滑性惩罚。将策略输出视为一个“高层目标”，由一个低级的、满足动力学约束的控制器（如 MPC 或轨迹生成器，见第4章）来执行。
