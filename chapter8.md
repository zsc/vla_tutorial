# 第8章 强化学习与微调：从指令遵循到策略优化

## 8.1 开篇段落

在前面的章节中，我们已经构建了一个强大的 VLA 预训练基座模型，它能够通过模仿学习（如行为克隆）对世界产生初步的理解和行动能力。然而，仅仅“模仿”是不够的。真实世界的任务要求智能体能够**优化**性能、适应未见过的场景、并从交互的后果中学习，而不仅仅是复制专家数据。本章将深入探讨如何通强化学习（RL）和相关微调技术，将我们的 VLA 模型从一个被动的“模仿者”转变为一个主动的“决策者”。

我们将系统地比较监督微调（SFT）、**强化微调 (RFT, 一种基于偏好学习的方法)** 和经典环境交互式强化学习的优劣与适用场景。我们将引用最新的实证研究，剖析 RL 究竟能在 VLA 的哪些泛化维度（视觉、语义、执行）上带来超越 SFT 的收益。在此基础上，我们将深入探讨策略优化的核心算法，并介绍**自反式指导**、**可验证奖励**等前沿技术。特别地，我们将借鉴 **VLA-R1** 和 **Seed1.5-Thinking** 等前沿工作中的经验，深入探讨在处理需要复杂推理（如长思维链 CoT）的任务时，如何设计数据、奖励模型和算法来克服 RL 训练的**不稳定性**。

本章的学习目标是，掌握一套将预训练模型转化为能够进行目标导向优化、同时兼顾安全与效率的高级策略的完整方法论，学会使用离线策略评估（OPE）在部署前科学地评估其潜在性能，为后续的仿真与 Sim-to-Real 迁移奠定坚实的策略基础。

---

## 8.2 文字论述

### 8.2.1 SFT vs. RFT vs. RL：记忆、偏好与泛化的权衡

从预训练基座模型出发，提升其特定任务能力的路径并非只有一条。选择哪条路径，取决于数据可用性、任务的性质以及对泛化能力的追求。

*   **监督微调 (Supervised Fine-Tuning, SFT) - 学习“做什么”**
    *   **核心思想**: SFT 本质上是**行为克隆 (Behavior Cloning, BC)** 的延续。它使用高质量的“状态-行动”或“指令-答案”对 `(s, a)` 数据集，通过最大化专家行为的对数似然来进行微调。它教模型**模仿**一个已知的“正确答案”。
        $$
        L_{SFT}(\theta) = -\mathbb{E}_{(s, a) \sim \mathcal{D}_{expert}} [\log \pi_\theta(a|s)]
        $$
    *   **优点**: 简单、稳定、对数据质量要求高但利用直接。是冷启一个策略的最佳起点。
    *   **缺点**:
        1.  **分布偏移 (Distribution Shift)**: 模型在执行中遇到微小未见过的状态，可能导致错误累积。
        2.  **次优天花板**: 模型的性能上限被专家数据所限制，无法发现超越专家的更优策略。实证研究表明，SFT 的性能会随着数据量的增加而饱和。

*   **强化微调 (Reinforcement Fine-Tuning, RFT) - 学习“怎么做”**
    *   **核心思想**: RFT 是一种基于**偏好学习 (Preference Learning)** 的高级微调技术。它承认了为复杂任务（如“以优雅的方式抓取”、“以安全的风格驾驶”）设计一个精确的奖励函数极其困难。因此，它不教模型模仿“正确答案”，而是通过奖励一个**行为偏好**来引导模型。其核心区别在于：SFT 教模型**“说什么/做什么 (What to say/do)”**，而 RFT 教模型**“怎么说/怎么做 (How to say/do)”**。
    *   **关键组件**:
        1.  **基础型 (Base Model)**: 我们想要微调的 VLA 模型。
        2.  **偏好数据集 (Preference Dataset)**: 这是训练“裁判”的数据。数据格式是**“prompt - chosen_completion - rejected_completion”** 的三元组。在 VLA 领域，这对应于“**状态/指令 - 更优的行动轨迹 - 较差的行动轨迹**”。
        3.  **奖励模型 (Reward Model, RM)**: 一个专门训练的模型，任务是给任何一个“(状态, 行动序列)”对打分。它通过学习偏好数据集，力求给 `chosen` 的行为打分高于 `rejected` 的行为。
    *   **工作流程三部曲**:
        1.  **准备偏好数据**: 针对你的目标（如更平滑、更安全），创建包含 `(state, chosen_action_sequence, rejected_action_sequence)` 的数据集。
        2.  **训练奖励模型**: 使用偏好数据训练 RM。其损失函数通常基于 Bradley-Terry 模型：
            $$
            L(\phi) = -\mathbb{E}_{(s, a_w, a_l) \sim \mathcal{D}_{pref}} [\log(\sigma(r_\phi(s, a_w) - r_\phi(s, a_l)))]
            $$
            其中 $a_w$ (winner) 是更优的行为，$a_l$ (loser) 是较差的行为。
        3.  **执行强化微调**: 使用 PPO 等 RL 算法，以 RM 的输出作为奖励信号，微调基础模型。模型通过生成不同行为并从 RM 获得反馈，逐渐学会生成能获得高分的行为模式。
    *   **VLA 典型用例**:
        *   **提升动作的平滑性与效率**:
            *   `prompt`: "将蓝色杯子放到水槽里"
            *   `chosen`: (一条平滑、直接的机械臂轨迹)
            *   `rejected`: (一条犹豫、多次停顿、有冗余移动的轨迹)
        *   **减少不必要的行为拒绝（在安全边界内更乐于助人）**:
            *   `prompt`: "在一个略显狭窄的空间中抓取物体"
            *   `chosen`: (一个经过仔细规划、成功避开障碍的轨迹)
            *   `rejected`: (模型判断过于保守，直接输出“无法执行”或静止不动)
        *   **提升驾驶风格的“拟人性”或“舒适度”**:
            *   `prompt`: (前方车辆减速的驾驶场景)
            *   `chosen`: (一个平滑、渐进的刹车动作)
            *   `rejected`: (一个突兀、让乘客不适的急刹车动作)

*   **环境交互强化学习 (RL from Environment Interaction) - 学习“为了什么做”**
    *   **核心思想**: 如果说 RFT 是从一个“品味裁判”（奖励模型）那里学习，那么这里的 RL 就是直接从“现实世界的后果”中学习。智能体直接与环境交互，接收由环境定义的、客观的标量奖励（如任务是否成功、是否发生碰撞、耗时多少），并以此优化策略。
    *   **优点与实证发现**:
        1.  **超越模仿**: 能够发现超越人类的、全新的解决方案。
        2.  **增强泛化 (来自《What Can RL Bring to VLA Generalization?》)**:
            *   **执行 (Execution) 泛化**: 收益**极其显著**。RL 通过试错探索，学会了如何从败中恢复，这是 SFT 和 RFT 都难以直接教授的。
            *   **语义 (Semantics) 泛化**: 收益**中等**。
            *   **视觉 (Vision) 泛化**: **与 SFT 持平**。
    *   **缺点**: **样本效率极低**，奖励函数设计是关键难点，且在线探索可能带来安全风险。

> **Rule-of-Thumb**:
> 1.  **起点 (SFT)**: 永远从 SFT/BC 开始，教会模型任务的**基本知识**（“做什么”）。
> 2.  **塑造品味 (RFT)**: 当你需要调整模型的**主观行为特性**（如风格、语气、安全偏好，“怎么做”）时，使用 RFT。
> 3.  **优化结果 (RL)**: 当你有明确的、可量化的**客观性能指标**和高保真仿真器时，使用环境交互式 RL 来压榨性能极限（“为了什么做”）。
> 4.  **混合使用**: 最佳实践是三者结合：用 SFT 初始化，用 RFT 对齐主观偏好，同时用环境稀疏奖励作为补充。

### 8.2.2 策略优化：核心算法与高效实践
(本节内容与上一保持一致，主要介绍 PPO、离线 RL 以及高效 PPO 的实践配方，这些是实现 RFT 和环境交互 RL 的底层算法。)

### 8.2.3 稳定化RL：来自 Seed1.5-Thinking 的前沿实践
(本节内容与上一版保持一致，介绍价值预训练、精细化优势估计、改进 PPO 裁剪和混合损失等高级稳定化技术。)

### 8.2.4 自反式指导与可验证奖励：RFT 的客观对应物

**RFT 通过学习人类偏好来处理主观、难以量化的目标。然而，在 VLA 领域，许多任务的“好坏”是可以被客观、自动地验证的。** 这就引出了与 RFT 互补的另一条强大路径：基于可验证奖励的 RL。

*   **自反式指导与 CoT**: **VLA-R1** 的工作完美诠释了这一点。
    1.  **SFT 注入思考模式**: 首先，在包含显式 `<think>...</think>` 推理链的 CoT 数据集上进行 SFT。
    2.  **RL 优化思考与行动**: 在 RL 阶段，模型生成的完整文本（思考+行动）将一并接受**可验证奖励**评估。

*   **设计可验证奖励 (Verifiable Rewards)**: **VLA-R1** 提出了一套精细化的、可自动验证的奖励函数：
    *   **轨迹奖励**: 采用**角度-长度增强 Fréchet 距离 (ALAF)** 来奖励路径的平滑度和动态一致性。
    *   **空间定位奖励**: 使用**广义交并比 (GIoU)** 代替标准 IoU，在不重叠时也能提供有意义的梯度。
    *   **格式奖励**: 一个二元奖励，确保模型的输出严格遵守预定结构。
    *   **消融研究证明**: VLA-R1 的消融实验清晰地表明，`SFT < SFT + CoT < SFT + CoT + RL`。这证明了**显式推理 (CoT)** 和**后续优化 (RL)** 两者相辅相成，缺一不可。

**这揭示了一个核心的设计模式：当目标是主观的，使用 RFT；当目标是客观的，设计可验证奖励。**

### 8.2.5 OPE（离线策略评估）：不上线如何知好坏
在将新训练的策略 $\pi_e$ 部署到昂贵或危险的真实环境中之前，我们迫切需要一种方法，利用已有的离线数据集（由旧的或行为策略 $\pi_b$ 收集）来评估 $\pi_e$ 的性能。

*   **重要性采样 (Inverse Propensity Scoring, IPS)**: 通过重要性权重 $\frac{\pi_e(a|s)}{\pi_b(a|s)}$ 来修正回报，但方差极大。
*   **双重鲁棒估计 (Doubly Robust, DR)**: 结合了基于模型的估计（如 Q 函数）和 IPS，显著降低了方差，是当前最实用的 OPE 方法之一。
*   **拟合Q评估 (Fitted Q-Evaluation, FQE)**: 直接在离线数据集上，为待评估的策略 $\pi_e$ 学习一个 Q 函数 $Q^{\pi_e}$。

> **Rule-of-Thumb**:
> 在进行任何 RL 微调后，**必须**运行 OPE。首选 DR 估计器来获得策略价值的点估计和置信区间。**永远不要完全信任 OPE 的结果**，它只是部署决策的辅助依据，而非最终判决。

---

## 8.3 本章小结

本章系统地阐述了如何将预训练 VLA 模型通过微调和强化学习，从一个模仿者提升为高效的决策者。我们明确了 SFT、RFT 和环境交互式 RL 的核心差异与适用场景。

*   **关键概念**:
    *   **微调谱系**: SFT 教会**“做什么”**，RFT 塑造**“怎么做”**，而环境交互式 RL 优化**“为了什么做”**。
    *   **偏好学习 (RFT)**: 通过**奖励模型**和**对比数据**，将主观的人类偏好转化为可优化的学习信号，是塑造模型行为风格和价值观的关键。
    *   **RL的非对称收益**: RL 对 VLA 的泛化能力提升主要体现在**执行鲁棒性**上。
    *   **可验证奖励**: 与 RFT 处理主观目标相对应，为客观、可量化的任务维度设计精确的奖励函数是提升 RL 效率的另一条关键路径。
    *   **稳定化与高效RL**: 采用共享主干、预热、最小化 Epochs 等高效实践，并结合价值预训练、改进 GAE 等高级技术来稳定训练过程。
    *   **离线评估 (OPE)**: 在部署前评估策略性能的必要工具，为决策提供数据支持。

---

## 8.4 常见陷阱与错误 (Gotchas)

1.  **奖励模型被利用 (Reward Model Exploitation)**
    *   **现象**: 这是 RFT 特有的风险。策略找到了一个方法来“欺骗”奖励模型获得高分，但产生的行为在人类看来却是无意义甚至错误的。这通常因为偏好数据覆盖的场景不够全面。
    *   **调试技巧**: 采用迭代式的数据收集和模型训练。定期用当前最优策略生成样本，让人类标注者找出其中“高分但低质”的例子，加入到偏好数据集中，然后重新训练奖励模型。

2.  **奖励黑客 (Reward Hacking)**
    *   **现象**: 在环境交互式 RL 中，智能体找到了一个最大化奖励的“捷径”，但这个行为完全违背了设计者的初衷。
    *   **调试技巧**: 使用多维度的、结构化的可验证奖励来减少漏洞。在训练过程中定期审查智能体的行为录像。

3.  **长程任务的RL训练崩溃**
    *   **现象**: 在需要长 CoT 或多步行动序列的任务中，PPO 训练过程非常不稳定。
    *   **调试技巧**: 系统性实施本章介绍的稳定化技术组合：从价值预训练开始，调整 GAE 和 PPO 裁剪策略，并加入正样本 LM 损失来防止语言能力退化。

4.  **对 OPE 结果的盲目乐观**
    *   **现象**: OPE 报告性能提升，但上线后表现反而下降。
    *   **调试技巧**: 始终报告 OPE 估计的**置信区间**。检查行为策略和评估策略的重合度。

5.  **灾难性遗忘 (Catastrophic Forgetting)**
    *   **现象**: 微调后，模型在原始任务上的性能大幅下降。
    *   **调试技巧**: 在微调数据中混入原始任务数据，并使用行为克隆正则化。

6.  **忽略策略输出的物理可实现性**
    *   **现象**: RL 策略输出了物理上无法执行的行动序列。
    *   **调试技巧**: 在奖励函数中加入对行动的平滑性惩罚，并将策略输出视为高层目标，由低级控制器执行。
