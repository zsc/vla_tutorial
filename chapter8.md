（交流可以用英文，所有文档中文，保留这句）

# 第8章 强化学习与微调（模型级）

> 目标：在已完成模态预训练与对齐的 V‑L‑A 基座上，完成**模型级强化学习微调**（SFT→RFT/RL），使策略既能**遵循指令**又能**优化实际任务指标**，同时满足**安全与可迁移**约束。本章强调奖励/偏好学习、KL 正则与行为约束、离线策略评估（OPE：IPS/DR/FQE），以及面向仿真与 Sim‑to‑Real 的**策略平滑与延迟补偿**。

---

## 内容导览

* 8.1 SFT vs. RFT vs. RL：记忆与泛化的权衡
* 8.2 策略优化：PPO/离线 RL/行为克制与 KL 正则
* 8.3 IFT/偏好学习：从指令与偏好到策略改进
* 8.4 自反式指导：CoT 评估、行动打分与自训练
* 8.5 奖励设计：成功率/安全/效率与奖黑客防范
* 8.6 数据效率：演示启动、DAgger 式纠偏与回放池
* 8.7 OPE（离线策略评估）：IPS/DR/FQE 三件套
* 8.8 稳健与安全：约束 RL、可恢复性与人机协同
* 8.9 面向落地的策略整形：平滑、延迟补偿与安全裕度
* 8.10 评测与消融：含“可迁移性探针”，服务于 Sim‑to‑Real

---

### 总体框架（ASCII 管线）

```
数据/演示 ──► 监督微调(SFT) ──► 参考策略 π_ref
                              │
偏好/奖励模型 r_φ  ◄──────────┤   （RLVR: 视觉-语言奖励）
                              ▼
      强化微调(RFT/RL) with KL(π||π_ref) + 约束
                              │
                   OPE: IPS/DR/FQE 校准
                              │
            策略整形(平滑/延迟补偿/安全裕度)
                              ▼
            仿真评测 ──► Sim-to-Real 准入/验收
```

> **Rule‑of‑thumb（全局）**
>
> 1. **先 SFT，后 RFT**：用 SFT 把“能听懂”做扎实，再用 RFT/RL 让策略“做对”。
> 2. **始终绑 KL 到参考策略**：$\mathrm{KL}(\pi|\pi_{\text{ref}})$ 是稳定器与控制阀。
> 3. **离线先跑 OPE**，在线再小步试错，**回放优先级覆盖长尾**。
> 4. **奖励 = 成功 + 安全 − 不适**（可分解项便于审计与调参）。
> 5. **部署前做“延迟与带宽审计”**：训练-部署时域一致性优先于任何 trick。

---

## 8.1 SFT vs. RFT vs. RL：记忆与泛化的权衡

**开篇**：SFT（监督微调）最像“示范记忆”；RFT（Reinforcement Fine‑Tuning）与 RL 则通过奖励信号推动**性能与偏好一致**。三者并非替代，而是层层递进。

* **SFT**：最小化交叉熵，学习 $\pi_{\text{SFT}}(a|s)$，优点是**稳定、数据效率高**；缺点是**目标退火**（imitate ≠ optimize）。
* **RFT/RL**：最大化期望回报 $\mathbb{E}*{\pi*\theta}[\sum_t \gamma^t r(s_t,a_t)]$，可追求**成功率/安全/舒适**等**任务指标**。
* **组合原则**：以 SFT 为**参考策略** $\pi_{\text{ref}}$，RFT/RL 时施加 KL 正则，使策略**在“能听懂”的稳定域内优化**。

> **Rule‑of‑thumb**：SFT 做“基线能力 + 风格对齐”，RFT/RL 只做“最后 20% 的任务优化”，并**量化 KL 漂移**（如相对熵预算）。

---

## 8.2 策略优化：PPO/离线 RL/行为克制与 KL 正则

**目标函数（带 KL 与熵）**：
[
\max_\theta;\mathbb{E}*{t}\Big[\min\big(r_t(\theta)A_t,;\mathrm{clip}(r_t(\theta),1-\epsilon,1+\epsilon)A_t\big)\Big]
-\beta,\mathrm{KL}!\left(\pi*\theta(\cdot|s_t),|,\pi_{\text{ref}}(\cdot|s_t)\right)
+\alpha,\mathcal{H}!\left(\pi_\theta(\cdot|s_t)\right),
]
其中 (r_t(\theta)=\frac{\pi_\theta(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)})。

* **KL 两种使用法**：
  (i) 罚项（上式中的 (\beta) 固定或自适应）；
  (ii) **KL‑control** 解释：最优化 $\mathbb{E}[r]-\frac{1}{\beta}\mathrm{KL}$，等价软约束。
* **行为克制（Behavior Regularization）**：对**离线/混合**数据，加入
  [
  \lambda_{\text{BC}},\mathbb{E}*{(s,a)\sim \mathcal{D}}\big[-\log \pi*\theta(a|s)\big],
  ]
  或采用 **CQL/IQL** 风格制 OOD 动作价值膨胀。
* **连续控制的动作平滑先验**：对轨迹加速度/跃度惩罚，或在解码端约束**带宽**（参见第 4 章）。

> **工程提示**：(\beta) 自适应调节使 **KL per‑token** 维持在目标区间（例如 0.1–0.3 nats），显著降低崩溃风险。

---

## 8.3 IFT/偏好学习：从指令与偏好到策略改进

* **偏好数据**：成对比较 ((\tau^+, \tau^-))，训练奖励模型 (r_\phi) 或直接进行**直接偏好优化**（DPO/IPO 类）。
* **偏好到奖励**：最大化
  [
  \log \sigma!\Big(\sum_t r_\phi(s_t,a_t)\ \text{for}\ \tau^+ - \sum_t r_\phi(s_t,a_t)\ \text{for}\ \tau^-\Big).
  ]
* **RLVR（Vision‑Language Reward）**：将**视觉‑语言判别器**或**多模态 LM 评分**蒸馏为 (r_\phi)，面向**无标注环境**给出可扩展奖励。
* **IFT（Instruction‑Follow Tuning）**：将**语言一致性**（指令遵从/风格/安全）和**任务完成度**写入统一奖励：
  (r = w_{\text{succ}}r_{\text{succ}} + w_{\text{lang}}r_{\text{IF}} - w_{\text{tox}}r_{\text{tox}})。

> **Rule‑of‑thumb**：偏好模型**先做标定**（温度/校准曲线），再用于长序列评分；把“语言一致性”与“物理达成”拆成**不同通道**的奖励，便于诊断。

---

## 8.4 自反式指导：CoT 评估、行动打分与自训练

* **自评估（Self‑Critique）**：让模型在动作后生成**解释/打分**，作为**辅助奖励**或过滤器。
* **过程监督（Process Reward）**：对“中间思考/分解步骤”评分，抑制**捷径与伪相关**。
* **不确定性驱动采样**：根据**价值方差/分歧度**挑选回放样本（优先级回放 + 反事实切片）。

> **实践要点**：把“解释”当作**可稽核产物**保存到回放元数据中，供 OPE 与故障回放调用。

---

## 8.5 奖励设计：成功率/安全/效率与奖黑客防范

**可分解奖励**（示例）：
[
r_t = w_s,\underbrace{\mathbb{1}[\text{成功}]}_{\text{终局}}

* w_c,\underbrace{\text{碰撞/越界/违规}}_{\text{硬约束}}
* w_j,\underbrace{\text{跃度/不适}}_{\text{舒适}}
* w_e,\underbrace{\text{能耗/耗时}}_{\text{效率}}

- w_x,\underbrace{\text{社交合规/礼让}}_{\text{交互}}
  ]

* **奖黑客**：对代理可能“投机”的信号做**反作弊**（如密度惩罚、检测器矛盾惩罚、可达性检查失败惩罚）。
* **CVaR/风险敏感**：最小化**分位损失** (\mathrm{CVaR}_\alpha) 提升长尾安全。

> **Rule‑of‑thumb**：先把**硬约束**（安全/合规）做成**屏蔽或不可行域投影**，再优化软目标；能“投影”的就别“罚”。

---

## 8.6 数据效率：演示启动、DAgger 式纠偏与回放池

* **演示启动（Demonstrations Bootstrapping）**：用专家或“几何教师”初始化 (\pi_{\text{ref}})。
* **DAgger/AGGREVATE 思想**：在线聚集**自身访问的状态分布**，请求教师纠偏，减轻**分布漂移**。
* **回放池治理**：

  * **分层缓存**：成功/失败/界样本分池；
  * **优先级**：基于 TD‑error、价值方差、发生稀有度；
  * **去重与漂移管控**：窗口化采样 + 版本化元数据。

> **工程建议**：回放条目附带**场景签名**（地图/天气/对手策略 hash），为 Sim‑to‑Real 的**复现实验**与**边界覆盖率**铺路。

---

## 8.7 OPE（离线策略评估）：IPS/DR/FQE 三件套

当在线试验代价高或有安全约束时，**OPE**提供离线估计新策略 (\pi) 的价值 (\hat{V}^\pi)。

**(1) IPS / PD‑IPS（逐决策重要性采样）**
行为策略 (\mu)，重要性比：
[
\rho_{0:t}=\prod_{k=0}^t \frac{\pi(a_k|s_k)}{\mu(a_k|s_k)}.
]
分步 IPS 估计：
[
\hat{V}*{\text{PD-IPS}}=\frac{1}{n}\sum*{i=1}^n \sum_{t=0}^{T-1}\gamma^t,\rho_{0:t}^{(i)},r_t^{(i)}.
]
**优点**：无模型偏差；**缺点**：方差高，对支持重叠极敏感。

**(2) Doubly‑Robust（DR）**
引入近似 ( \hat{Q}(s,a),\hat{V}(s) )：
[
\hat{V}*{\text{DR}}=\frac{1}{n}\sum*{i=1}^n\sum_{t=0}^{T-1}\gamma^t\Big[
\rho_{0:t}^{(i)}\big(r_t^{(i)}-\hat{Q}(s_t^{(i)},a_t^{(i)})\big)
+\rho_{0:t-1}^{(i)}\hat{V}(s_t^{(i)})\Big].
]
**性质**：模型或重要性权任一正确即无偏；常配合截断权重与控制变元降方差。

**(3) FQE（Fitted Q Evaluation）**
固定策略 (\pi)，拟合 (Q^\pi) 解
[
\min_\psi \mathbb{E}*{(s,a,r,s')\sim \mathcal{D}}\Big[
Q*\psi(s,a) - \big(r + \gamma,\mathbb{E}*{a'\sim \pi(\cdot|s')}[Q*{\bar\psi}(s',a')]\big)
\Big]^2,
]
再以 (\hat{V}^\pi=\mathbb{E}*{s\sim d_0}\mathbb{E}*{a\sim\pi}[Q_\psi(s,a)]) 估计。
**优点**：方差较低、可诊断；**关键**：**覆盖度**与**函数近似误差**。

> **OPE Rule‑of‑thumb**：
>
> * **先 DR 再 FQE**：DR 做快速筛查，FQE 做深度评估与剖析。
> * **报告置信条**：自举（bootstrap）+ 截断比（如 (c=10)）的**敏感性曲线**必须呈现。
> * **检查支持重叠**：估计 (\Pr_\mu(a|s)) 低于阈值则标红（OOD 风险）。

---

## 8.8 稳健与安全：约束 RL、可恢复性与机协同

* **CMDP/拉格朗日**：约束 ( \mathbb{E}*\pi[\sum c_t] \leq C )，解
  [
  \max*\pi \min_{\lambda\ge0}\ \mathbb{E}_\pi\Big[\sum_t \gamma^t(r_t-\lambda c_t)\Big] + \lambda C.
  ]
* **RTA（运行时保障）**：训练期**不放松的硬约束**→ 部署期**屏蔽/投影**（QP/CBF/STL‑Shield）。
* **可恢复性（Recovery）**：学习**恢复策略** (\pi_{\text{rec}}) 与**安全集合**，结合**危险率估计**触发切换。
* **人机协同**：在不确定阈高的状态维持**共享控制**，将**干预**记录入回放以改进策略。

> **Rule‑of‑thumb**：让“约束惩罚”只作为**训练信号**，把**合规**交给**屏蔽/投影**；训练期就按部署期的**同一屏蔽逻辑**执行。

---

## 8.9 面向落地的策略整形：平滑、延迟补偿与安全裕度

**时域一致性问题**（训练—部署）是现实落地的“隐形杀手”。

* **采样/执行延迟**：观测 (s_t) → 动作 (a_{t+\Delta})。用 **Smith 预估/延迟补偿** 或**并行预测多步动作**降低相位滞后。
* **带宽/平滑**：在解码端加**谱域正则**或在输出后级接**低带宽安全轨迹生成器**（第 4.11 节接口）。
* **安全裕度**：把 OOD/高不确定性区域映射到**保守动作集**；对阈值（距离/速度/时距）设**安全系数** (>1)。
* **温度与采样策略**：训练时的**温度/Top‑p**与部署一致；对**确定性控制**优先采用**均值/投影**而不是采样。

> **调参准则**：离线重放 + 台架（HIL）测出**延迟‑相位裕度曲线**，再定平滑与裕度；**先稳再快**。

---

## 8.10 评测与消融：含“可迁移性探针”，服务于 Sim‑to‑Real

* **指标分层**：

  1. **成功**（达成率/里程碑）；
  2. **安全**（碰撞/违规/最小时距）；
  3. **舒适**（加速度/跃度统计）；
  4. **社交合规/公平**（第 10 章对应）；
  5. **可迁移性探针**（对传感/动力学扰动的降级曲线）。
* **消融**去掉 KL、去掉偏好、去掉屏蔽、去掉平滑，观察**退化形态**。
* **可解释诊断**：对失败轨迹生成**反事实**（若更保守/若更靠右…）、显示**奖励分解与约束激活**时间线。

> **交付建议**：评测脚本输出**覆盖率报告**（场景簇×扰动等级×成功率），为第 11 章的**准入清单**直接复用。

---

## 本章小结

1. **SFT→RFT/RL 的两阶段**：用 SFT 固化语言/风格与基本能力，RFT/RL 绑定 KL 进行任务优化。
2. **偏好与 RLVR**：多模态偏好/奖励把“语言一致性 + 物理成功”统一在奖励层。
3. **OPE 三件套（IPS/DR/FQE）**：离线先估计、带置信条与支持检查，再小步在线。
4. **稳健与安全**：CMDP + 屏蔽/投影（RTA），并训练可恢复策略。
5. **落地整形**：平滑、延迟补偿、安全裕度与采样一致性是 Sim‑to‑Real 的桥面铺装。

---

## 常见陷阱与错误（Gotchas）与调试技巧

1. **KL 失控 → 策略崩溃**

   * *症状*：训练回报飙升但离线/线上崩。
   * *技巧*：设 **KL 目标环**（per‑token 0.1–0.3 nats），(\beta) 自适应；监控 **支持重叠**热力图。
2. **奖励黑客**

   * *症状*：策略“刷分”但不达成真实目标。
   * *技巧*：奖励**分解与可视化**；引入**可达性/一致性检查**和**矛盾检测器**；对探测器加入**反事实审计**。
3. **离线分布外动作膨胀**

   * *症状*：FQE 乐观，线上不稳。
   * *技巧*：行为正则（BC/CQL/IQL），**动作投影**到可行域；FQE 做**保守估计**与不确定性报告。
4. **训练‑部署时域不一致**

   * *症状*：仿真稳，实机振荡/超调。
   * *技巧*：统一**采样率/延迟**；Smith 预测/多步并行；**轨迹后端平滑器**替代前端“软罚”。
5. **偏好模型未校准**

   * *症状*：长序列评分漂移，RFT 方向错误。
   * *技巧*：**温度标定**、E‑Brier/ECE；对**多通道奖励**分别做灵度扫描。
6. **OPE 误导**

   * *症状*：离线估好，线上掉坑。
   * *技巧*：DR + FQE **交叉验证**；报告**权重截断敏感性**；用**场景签名**做分簇评估。
7. **安全约束用“罚”替代“屏蔽”**

   * *症状*：训练 OK，部署仍违规。
   * *技巧*：把**硬约束**实现为**投影/屏蔽**（RTA），与训练环境**同构**。
8. **采样温度不一致**

   * *症状*：离线评测温和，部署随机发散。
   * *技巧*：部署期**固定温度/Top‑p**或用**确定性解码**；与 OPE 评测设定一致。
9. **回放池污染与遗忘**

   * *症状*：灾难性遗忘，或过拟合近期数据。
   * *技巧*：窗口化 + 分层缓存 + 优先级回放；定期**冷启动抽检**历史关键场景。
10. **指标只看均值**

    * *症状*：平均好，长尾差。
    * *技巧*：报告 **CVaR/分位** 与**最坏 5%**；绘制扰动‑成功率**降级曲线**。

---

## 附：关键公式与记忆卡

* **PPO（带 KL/熵）**：见 §8.2。
* **偏好到奖励（对比）**：
  (\max_\phi \sum \log \sigma\big(R_\phi(\tau^+)-R_\phi(\tau^-)\big))。
* **DR OPE**：见 §8.7。
* **CMDP 原始‑对偶**：(\max_\pi \min_{\lambda\ge0}\mathbb{E}_\pi[\sum \gamma^t(r_t-\lambda c_t)]+\lambda C)。
* **策略整形**：部署端**投影/屏蔽** + **低带宽轨迹器** + **延迟补偿**。

---

### 本章完成后应能回答

* 何时用 SFT、何时切入 RFT/RL？KL 目标选多少？
* 如何把视觉‑语言评分做成可用的 RL 奖励（RLVR）？
* 在不做在线试验前，如何用 IPS/DR/FQE 可靠地做离线价值估计？
* 策略部署前应如何做**延迟与带宽审计**并设置**安全裕度**？

> **下一章预告**：第 9 章将在仿真中让策略**真正交互**，并把本章的奖励/偏好/OPE 管线接入**环境闭环**，为多智能体与 Sim‑to‑Real 铺路。

