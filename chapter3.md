# 第3章 语言模态：符号推理、过程编排与系统调度

## 开篇段落

如果说视觉模态为智能体提供了“看清世界”的原始数据，行动模态定义了其“改造世界”的物理能力，那么语言模态则扮演着连接两者的“中枢神经系统”。本章将深入探讨语言在 Visual-Language-Action (VLA) 模型中的核心角色，我们不再将其仅仅视为文本生成器，而是作为一个强大的**符号推理引擎、过程编排器和系统调度中心**。学习本章后，你将能够理解并运用大语言模型（LLM）的几大核心能力——**思维链（Chain-of-Thought）**进行分步推理，**记忆机制**实现状态维持与知识检索，以及**工具调用**来扩展模型的能力边界。最终，我们将把这些能力整合，展示语言如何作为VLA系统的“主线程”，为感知和行动提供可解释的组织与约束，并在迈向部署的过程中，承担起策略解释、人机协同与安全保障的关键职责。

## 文字论述

### 3.1 语言的“智性”地位：符号操作与抽象表达

在VLA三元组中，语言拥有独特的地位。视觉（像素强度）和行动（关节角度、力矩）本质上是连续的、高维的、**次符号（sub-symbolic）**信号。而语言，从乔姆斯基的生成语法到福多的思想语言假，其核心是离散的、符号化的系统。这种符号性赋予了语言两大无可比拟的优势：

1.  **组合性（Compositionality）**：有限的词汇和语法规则可以生成无限的、有意义的句子。这使得语言能够以极高的效率描述新颖的场景和复杂的任务。例如，我们可以轻松理解“把桌子上那本蓝色封面的、立着的书中第三页的红色回形针取出来”，尽管我们可能从未见过这个确切的组合。
2.  **抽象能力（Abstraction）**：语言能够将复杂的感知模式压缩成一个简洁的符号，如用“椅子”指代各种形态各异的坐具。这使得在高层级进行规划成为可能，而不必陷入底层像素和物理细节的泥潭。

一个复杂的任务，如“把桌上的红苹果放到冰箱里”，在语言层面可以被清晰地分解为一系列符号化的子任务：
1.  `locate(object: 'apple', attribute: 'red', location: 'table')`
2.  `grasp(object: 'red_apple')`
3.  `navigate(destination: 'refrigerator')`
4.  `open(object: 'refrigerator')`
5.  `place(object: 'red_apple', destination: 'inside_refrigerator')`
6.  `close(object: 'refrigerator')`

现代LLM通过在海量文本上进行“预测下一个词元”的预训练，隐式地学习到了这种符号操作的强大统计规律。然而，VLA的核心挑战在于**语义接地（Semantic Grounding）**：如何将这些抽象的符号（如`'apple'`）与视觉模态中具体的像素区域以及行动模态中精确的物理操作建立起稳定可靠的联系。本章后续内容将围绕如何利用LLM的结构化能力来应对这一挑战展开。

### 3.2 Chain-of-Thought：分步推理与错误驱散

对于需要多步逻辑才能解决的问题，直接从输入到输出的“一步到位”式映射（`Input -> Output`）往往效果不佳，因为这要求模型在单次前向传播中隐式完成所有推理步骤，难度极大。**思维链（Chain-of-Thought, CoT）**的出现是该领域的一个里程碑。

**代表性工作与演进**：
*   **Wei et al. (2022) 的《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》** 开创性地提出，只需在提示（Prompt）中加入一些引导模型进行分步思考的范例（Few-shot CoT），就能显著提升LLM在算术、常识和符号推理任务上的表现。
*   **Kojima et al. (2022) 的 Zero-shot CoT** 发现，甚至不需要提供范例，只需在提示末尾加上一句简单的“Let's think step by step.”，就能诱导出模型的推理能力，极大地降低了CoT的使用门槛。
*   **Yao et al. (2023) 的 Tree-of-Thoughts (ToT)** 和 **Besta et al. (2023) 的 Graph-of-Thoughts (GoT)** 进一步将线性推理链扩展到树状或图状结构，允许模型在每个步骤探索多个可能性、进行自我评估和回溯，从而解决更复杂的规划和搜索问题。

**CoT为何有效？** 它将隐式的、难以捉摸的推理过程**外部化（externalize）**为可见的文本。生成的每一步推理都成为下一步生成的上下文，这相当于在自回归生成的过程中为模型提供了更长的“计算时间”和更丰富的中间状态，从而降低了解决问题的认知负荷。

**ASCII 图示：CoT 工作流**
```
+----------------+   +---------------------------------+   +------------------+
| Visual & Text  |   |                                 |   |                  |
|   Observation  |-->|      LLM with CoT Prompt        |-->|  Structured Plan |
| e.g., "水洒了"  |   |                                 |   | e.g., find_rag,  |
+----------------+   +---------------------------------+   |      wipe_table  |
                         |
                         | Generated Text (The "Thought"):
                         | "1. 识别到桌上有水渍，这是一个需要清理的状态。
                         |  2. 清理液体通常需要吸收性材料。
                         |  3. 在当前环境中，抹布或纸巾是最佳选择。
                         |  4. 因此，我的计划是：首先找到抹布，然后用它擦拭桌子。"
                         |
```
数学上，传统的策略 $A = \pi(O)$ 被分解为一个更精细的自回归过程。令 $C = (c_1, c_2, ..., c_T)$ 为思维链，其中 $c_t$ 是第 $t$ 步的推理文本：
$$
c_t \sim p(c_t | O, c_{<t}) \\
A \sim p(A | O, C)
$$
这清晰地表明，每一步思考都依赖于原始观测和之前的思考，最终的行动则以完整的思考过程为条件。

**Rule-of-thumb**:
*   对于任何需要超过两步逻辑的VLA任务，优先使用CoT。宁可让模型输出“啰嗦”的中间步骤，也不要追求简洁的“一步到位”式答案。这些中间步骤是调试和验证模型行为逻辑的宝贵线索。
*   CoT会增加推理延迟。在对延迟敏感的应用中，可以考虑**CoT蒸馏**：使用大型模型生成带有CoT的“教师”数据，然后训练一个更小的“学生”模型，让它直接学习从观测到最终行动的映射，从而在部署兼顾性能与速度。

### 3.3 记忆与抽象：压缩、检索与情境绑定

LLM的上下文窗口（Context Window）提供了宝贵的**短期工作记忆**，但其长度有限且计算成本高昂（Transformer的注意力机制通常是 $$O(L^2)$$ 复杂度，其中 $$L$$ 是序列长度）。对于需要长期交互、依赖海量外部知识或跨越多个任务周期的智能体而言，必须建立**长期记忆**机制。

**代表性工作**：
*   **Lewis et al. (2020) 的 Retrieval-Augmented Generation (RAG)** 模型为此提供了经典范式。它将LLM的参数化隐式知识（存储在模型权重中）与外部知识库的非参数化显式知识结合起来，实现了“开卷考试”。

**记忆的类型与实现**：
在VLA中，记忆库可以被系统地组织为：
1.  **情景记忆（Episodic Memory）**：存储过去的`(观测, 行动, 结果)`序列。这对于从失败中学习（例如，“上次我这样抓杯子，它滑掉了”）和识别重复模式至关要。实现方式通常是向量数据库，将历史片段编码为嵌入向量。
2.  **语义记忆（Semantic Memory）**：存储关于世界的事实性知识。例如，`（冰箱, is_a, 容器）, （扳手, used_for, 拧螺母）`。这可以由知识图谱或简单的文本块构成。
3.  **程序记忆（Procedural Memory）**：存储解决特定问题的“技能”或子程序。成功的CoT轨迹或行动序列本身就可以被存储为可复用的“脚本”。

**ASCII 图示：RAG 工作流**
```
+-------------+   +-------------------------+      +----------------+   +-----------------------+
| User Query/ |   |                         |      |  Retrieved     |   |                       |
| Observation |-->| Retriever (e.g., BM25,  |----->|  Documents/    |-->| LLM Prompt            |
| e.g., "帮我找 |   | Sentence-BERT)          |      |  Memories      |   | (Query + Retrieved    |
| 上次用的扳手"|   +-------------------------+      +----------------+   |  Context)             |
+-------------+         |                                              +-----------------------+
                        | Index                                                   |
                        V                                                         V
                +-----------------+                                     +-------------------+
                | Vector Database |                                     |  Generated Answer |
                | (Episodic/Sem.) |                                     | "根据记忆，扳手最后 |
                +-----------------+                                     | 出现在红色工具箱里。" |
                                                                      +-------------------+
```

**Rule-of-thumb**:
*   不要将LLM的上下文窗口误认为真正的长期记忆。对于需要跨越会话、依赖特定领域知识或从长历史中学习的任务，优先考虑基于检索的RAG架构。
*   检索器的质量至关重要。“圾进，垃圾出”。需要投入精力优化检索模型（例如，对领域特定的文本进行微调），并设计良好的索引和分块策略。
*   权衡检索频率与延迟。并非每一步决策都需要检索。可以设计触发机制，例如当模型输出低置信度token，或显式生成一个`[RETRIEVE]`标记时，才启动检索过程。

### 3.4 工具调用：从 ReAct 框架到结构化函数调用

LLM本身是封闭的，不具备与外部世界交互、执行精确计算或访问实时信息的能力。**工具调用（Tool Calling）**机制将LLM从一个“夸夸其谈者”转变为一个能“动手实干”的指挥官。

**代表性工作与演进**：
*   **Yao et al. (2022) 的 ReAct (Reasoning and Acting)** 框架是这一思想的典范。它并非简单地让LLM调用工具，而是开创性地将CoT的“思考”与工具调用的“行动”**交织（interleave）**在一起，形成一个**“思考-行动-观察”**的智能体循环。这个循环使LLM能够根据工具执行的实时反馈来动态调整其后续的思考和行动。

**ASCII 图示：ReAct 循环**
```
+----------+                               +-----------------------+
|  Query:  |--> Thought 1: "我需要知道A城市 |                       |
| "A城市和B |    的天气。我应该先查A城市。"  |--> Action 1:           |
| 城市哪个热"|                                | call_weather_api(     |
+----------+                                | city='A')             |
                                            +-----------------------+
                                                  | (Tool Execution)
                                                  V
+------------------------------------+   +-----------------------+
| Observation 1: "API返回: A市25°C"  |<--| Thought 2: "好的，A市 |
+------------------------------------+   | 是25度。现在我需要查B市"|--> Action 2:           |
     |                                   +-----------------------+   | call_weather_api(     |
     V                                                               | city='B')             |
+------------------------------------+                             +-----------------------+
| Observation 2: "API返回: B市30°C"  |<----------------------------------+ (Tool Execution)
+------------------------------------+
     |
     V
Thought 3: "B市30度，A市25度。
B市更热。我可以回答问题了。"
     |
     V
Final Answer: "B城市更热。"
```
ReAct的早期实现依赖于LLM生成特定格式的文本（如 `Action: search[query]`），这需要对输出进行脆弱的字符串解析。为了解决这一工程问题，现代LLM（如GPT-4, Gemini, Claude 3）推出了更鲁棒的**函数调用（Function Calling）**功能。开发者可以在API请求中定义一组可用函数的schema（函数名、参数、描述），模型会生成一个结构化的JSON对象来指定调用哪个函数及传递什么参数，这极大地提升了可靠性，是ReAct思想在生产境中的成熟落地。

#### 3.4.1 思考预算与推理深度的自adaptive调度
一个关键的工程问题是：模型应该进行多少步思考或调用多少次工具？这引出了**思考预算（Thinking Budget）**的概念。
*   **静态策略**：设置固定的最大步数或时间限制。简单但缺乏适应性。
*   **动态策略**：
    *   **基于置信度的终止**：当模型对其最终答案的置信度超过某个阈值时停止。
    *   **元认知学习**：训练一个独立的策略或让LLM自身学会何时生成一个特殊的`[FINISH]`标记来结束推理循环。
    *   **基于价值的调度**：在每个步骤，模型可以估算继续思考一步所带来的“信息价值增益”（Value of Information），并与计算成本进行比较，从而决定是否继续。

**Rule-of-thumb**:
*   工具的接口设计应遵循软件工程的最佳实践：**原子化（Atomic）**、**幂等（Idempotent）**、**可观测（Observable）**。一个工具只一件事，重复调用结果相同，并返回清晰的成功或错误信息。
*   在工具的描述（docstring）中，不仅要说明它“做什么”，还要说明“什么时候用”、“输入输出是什么格式”以及“常见的失败原因”。这是指导LLM正确使用工具的最重要信息。

### 3.5 VLA 编排：三大主流范式

综合以上能力，语言模态在VLA系统中扮演了最终的编排者角色。目前，业界主要沿着三条技术路线来构建这一“大脑”，它们在处理速度、可解释性和任务类型上各有侧重。

#### 范式 A: 语言作为显式规划器 (Language as an Explicit Planner)

这是目前最主流的范式，其核心思想是利用LLM强大的CoT和工具调用能力，将一个复杂的VLA任务分解成一个**显式的、符号化的行动计划**。

在这个范式下，LLM如同一个项目经理：
1.  **接收输入**: 获取来自视觉模态的场景描述（如物体列表及其属性）、用户的指令历史记忆。
2.  **推理规划**: 通过CoT进行多步推理，决定下一步应该做什么。如果需要额外信息，就调用工具（如RAG检索记忆，或调用外部API）。
3.  **输出指令**: 生成一个结构化的、确定性的指令（如JSON或函数调用），交给下游的行动模块去执行。

**ASCII 图示：范式 A 决策流**
```
+---------------+   +-------------------+   +------------------------------------+
|               |   |                   |   |         LLM as Planner             |
| Visual Sensor |-->| Vision Encoder    |-->| Prompt: "You see [Scene]. Your     |
| (Raw Pixels)  |   | (to Scene Graph)  |   | goal is [Task]. Plan your steps."  |
+---------------+   +-------------------+   +------------------------------------+
                                                         | (Generates CoT, uses RAG/Tools)
                                                         V
+---------------+   +-------------------+   +------------------------------------+
|               |   |                   |   | Structured Command (e.g., JSON)    |
| Action Signal |<--| Action Decoder    |<--| { "action": "move_to", "params":   |
| (Joint Torques)|   | (Trajectory Gen.) |   |   {"target": "red_apple"} }        |
+---------------+   +-------------------+   +------------------------------------+
```
**优势**: 可解释性强（CoT日志即为决策理由），易于集成复杂工具和知识，适合处理需要长时程、多步骤逻辑的离散任务。

#### 范式 B: 语言作为隐式世界模型 (Language as an Implicit World Model)

与上述将语言作为显式符号规划器的主流范式不同，另一条极具影响力的路线将语言视为一种**丰富的观测信号**，用以构建一个**多模态世界模型（Multimodal World Model）**。其核心思想并非用语言生成行动计划，而是**用语言来更好地预测世界的未来**。

**代表性工作**:
*   **Lin et al. (2024) 的 Dynalang** 是该范式的杰出代表。它建立在 DreamerV3 这样的模型之上，其核心理念是：智能体不应将语言（如“这个按钮是开灯的”）直接映射到行动，而应将其理解为一条有助于**预测未来观测**的线索。如果智能体按下那个按钮，它应该能预测到未来的视觉观测中，灯会变亮。

**ASCII 图示：范式 B (Dynalang-style) 决策流**
```
          +-------------------------------------------------+
          |                  World Model                    |
          |                                                 |
(h_t,z_t) |  h_{t+1} = RNN(h_t, z_t, a_t)  <-- Action a_t ---+---- Actor(h_t,z_t)
   ^      |  z_{t+1} ~ Transition(h_{t+1})                  |
   |      |                                                 |
   |      |  (x_hat, l_hat, r_hat) = Decoder(h_{t+1},z_{t+1})|
   |      +-------------------------------------------------+
   |                                 ^
   +---- z_t ~ Encoder(x_t, l_t, h_t) | (Learning Signal: Reconstruction & Prediction)
         ^           ^
         |           |
+--------+---+   +---+--------+
| Vision x_t |   | Language l_t |
+------------+   +------------+
```
**优势**: 能够以端到端的方式处理更广泛、更多样化的语言（不仅仅是指令），与低层控制结合更紧密，更适合需要快速反应的动态环境。

#### 范式 C: 语言作为流式脚手架 (Language as a Streaming Scaffold)

这是一条新兴的前沿路线，它试图结合前两种范式的优点，特别是在**低延迟、连续生成的场景**下。其核心思想是：语言不产生宏观计划，也不仅仅是观测，而是作为一个**与行动紧密耦合的、逐时间步生成的中间表征**，像脚手架一样引导着最终行动的生成。

**代表性工作**:
*   **Défossez et al. (2024) 的 Moshi** 在实时语音对话中完美诠释了这一思想。其 **"Inner Monologue"** 机制，在生成每一个音频（行动）词元之前，会先在内部生成一个**时间对齐的文本（语言）词元**作为前缀。这形成了一个流式的、层次化的生成过程。

**ASCII 图示：范式 C (Moshi's Inner Monologue) 决策流 (at timestep s)**
```
+-------------------------------------------------------------------------+
|                Hierarchical Generation at Timestep s                    |
|                                                                         |
|  +----------------+   +---------------+   +---------------------------+ |
|  |                |   |               |   | Generates Text Token W_s  | |
|  | Temporal Context |-->| Depth         |-->| (e.g., "Hel-")            | |
|  | from t < s     |   | Transformer   |   +-------------+-------------+ |
|  |                |   |               |                 |               |
|  +----------------+   +---------------+                 V               |
|                                         | Generates Semantic Token A_{s,1}|
|                                         +-------------+-------------+ |
|                                                       |               |
|                                                       V               |
|                                         | Generates Acoustic Tokens   |
|                                         | (A_{s,2}, ..., A_{s,Q})       |
|                                         +---------------------------+ |
+-------------------------------------------------------------------------+
```
这个过程在每个时间步（例如每80毫秒）重复，语言词元`W_s`为后续的行动词元`A_s`提供了即时的、本地化的语义约束。这与范式A的“先完成整个文本计划再执行”形成鲜明对比，也比范式B的纯粹预测更具结构性。

**优势**:
*   **极低延迟**: 能够在流式交互中保持语言的引导作用，避免了宏观规划带来的延迟。
*   **高质量生成**: 文本“脚手架”显著提升了后续行动（如语音）的语言质量和事实一致性。
*   **架构统一**: 通过调整文本和行动词元之间的延迟，可以零样本（zero-shot）地将同一个模型用于文本到行动（TTS）或行动到文本（ASR）的任务。

**Rule-of-thumb**:
*   **选择范式A（显式规划器）** 当你的任务是**长时程、分阶段、需要调用多种外部工具、且对可解释性要求极高**时（例如，复杂的机器人管家任务、软件操作）。
*   **选择范式B（隐式世界模型）** 当你的任务是**在动态环境中进行连续控制、需要紧密的感知-行动循环、且语言以非指令形式提供上下文**时（例如，根据实时环境描述导航）。
*   **选择范式C（流式脚手架）** 当你的任务是**需要极低延迟的实时交互、连续生成复杂行动序列、且行动质量高度依赖于语言连贯性**时（例如，实时对话系统、人机协同演奏）。

### 3.6 安全与稳健：幻觉抑制、可解释提示与审计

语言模型的**幻觉（Hallucination）**在VLA中是致命的，它可能导致生成物理上不可能、不安全或不合逻辑的行动。
**缓解策略**：
*   **感知植根（Perceptual Grounding）**：强制模型的输出必须与视觉观察有明确的对应关系。例如，在生成`grasp(object_id='red_cup')`之前，系统必须确认视觉模块确实检测到了一个ID为`'red_cup'`的物体。
*   **引用与溯源（Citation & Traceability）**：要求模型在生成计划时，必须引用其信息来源。例如，“`Action: move_to(kitchen) because: the user asked for water [Source: User Instruction] and the kitchen contains a sink [Source: Semantic Memory].`”
*   **输出验证与过滤**：在LLM的输出和行动解码器之间设置一个验证层。该层可以检查生成的计划是否符合预定义的语法、是否违反已知的物理或安全约束（例如，不能命令机械臂移动到其运动范围之外）。
*   **审计日志**：完整记录LLM的输入提示、CoT过程、工具调用及返回结果。这是事后分析失败、进行持续改进宝贵资产。

### 3.7 面向部署：人机协同与策略可解释

在真实世界部署中，VLA系统需要与人类协作并接受监督。语言模态是实现这种交互的最佳接口。
*   **策略解释**：系统可以将其行动计划用自然语言向人类解释（“我打算先拿起杯子，然后走到饮水机旁”），在执行高风险操作前获取许可。
*   **在线干预**：人类监督员可以通过自然语言指令（“不，先拿那个蓝色的，它更稳固”）实时修正系统的行为。
*   **Sim-to-Real 中的桥梁作用**：在仿真中，可以用语言来标注复杂的场景和任务，并解释策略失败的原因。当迁移到现实世界时，如果遇到域差异（如新的物体、不同的光照），系统可以用语言描述未曾见过的现象（“我看到一个以前没见过的闪光物体”），从而请求人类帮助或触发预设的安全降级策略。

## 本章小结

本章系统阐述了语言模态作为VLA系统“脑”的核心功能。我们超越了传统的文本生成视角，将其定位为符号推理、过程编排与系统调度的中心。

*   **核心理念**：语言通过其**符号性**和**抽象能力**，为处理复杂、长期的任务提供了逻辑框架，但必须通过**语义接地**与物理世界相连。
*   **三大主流范式**：
    *   **显式规划器 (Explicit Planner)**: 以 `ReAct` 为代表，利用 CoT 和工具调用生成可解释的符号计划，适用于长时程、结构化的任务。
    *   **隐式世界模型 (Implicit World Model)**: 以 `Dynalang` 为代表，将语言作为预测未来世界状态的丰富信号，适用于动态、连续的控制任务。
    *   **流式脚手架 (Streaming Scaffold)**: 以 `Moshi` 的 "Inner Monologue" 为代表，逐时间步生成文本作为行动前缀，实现了极低延迟下的高质量连续生成。
*   **关键技术演进**：
    *   **思维链 (CoT)**：从线性的CoT发展到更复杂的**树状/图状思 (ToT/GoT)**。
    *   **记忆机制 (RAG)**：通过外部知识库的检索增强，实现了长期记忆。
    *   **工具调用 (ReAct/Function Calling)**：从文本解析演进到**结构化的函数调用**，提升了可靠性。
*   **部署价值**：其生成的可解释日志为**安全审计**提供了基础，其自然语言接口是实现高效**人机协同**与**Sim-to-Real适应**的关键。

## 常见陷阱与错误 (Gotchas)

1.  **提示脆弱性 (Prompt Fragility)**：
    *   **陷阱**：LLM的输出对提示的微小变化（如一个词、一个标点、范例的顺序）极其敏感，导致行为不稳定。
    *   **调试与规避**：建立一个标准化的评估集来量化不同提示模板的性能。使用结构化提示格式（如XML标签）来明确区分指令、上下文和范例。开发“提示模板优化器”，通过自动化测试寻找最鲁棒的提示。

2.  **级联错误 (Cascading Errors)**：
    *   **陷阱**：在长思维链或多步工具调用中，第一步的微小错误（如错误识别物体）会被后续步骤不断放大，最终导致整个任务失败。
    *   **调试与规避**：实施**闭环修正**。每执行一步行动后，将新的观测反馈给LLM，让它评估当前状态与计划的偏差，并决定是继续原计划、修正计划还是请求帮助。这从开环的“计划-执行”模式转变为闭环的“感知-计划-行动-评估”循环。

3.  **工具滥用或死循环**：
    *   **陷阱**：模型可能会陷入反复调用同一个工具但问题依旧无解的循环中，或者对一个简单问题使用过于复杂的工具。
    *   **调试与规避**：为推理循环设置明确的**最大步数限制**和**工具调用频率限制**。在工具的反馈中不仅包含结果，还应包含执行耗时和资源消耗，让LLM学会权衡收益与成本。

4.  **语义漂移与“抽象税” (Semantic Drift & Abstraction Tax)**：
    *   **陷阱**：将LLM生成的自然语言描述接等同于精确的物理或几何指令。例如，模型说“向左移动一点”，但“一点”是多少？同时，高级语言推理带来的计算延迟（“抽象税”）可能使系统无法应对需要快速反应的动态情况。
    *   **调试与规避**：设计一个分层控制架构。LLM负责**高层策略**（做什么），而一个快速、反应式的**底层控制器**负责**底层执行**（怎么做）。底层控制器将“向左移动一点”这样的模糊指令，结合当前物理状态，翻译成安全、平滑的轨迹。

5.  **上下文污染 (Context Contamination)**：
    *   **陷阱**：在长时程任务中，过时的或不相关的信息会堆积在LLM的上下文中，干扰其对当前任务的判断。例如，之前的任务失败信息可能会让模型在当前无关任务中表现得过于保守。
    *   **调试与规Версия**：实现一个主动的**上下文管理器**。该管理器可以根据当前任务目标，从长期记忆选择性地将最相关的信息注入到提示中，并定期清理过时的短期记忆。可以训练一个小型模型来专门做这项“注意力管理”工作。
