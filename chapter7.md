（交流可以用英文，所有文档中文，保留这句）

# 第7章 预训练：模态预训练与跨模态对齐

> **开篇段落（学习目标）**
> 本章围绕 **两阶段**策略构建 V‑L‑A 基座模型：**模态内预训练**（视觉/语言/行动）→ **跨模态对齐预训练**（V–L、L–A、V–A）。你将掌握：如何设计**训练日程（Curriculum）**、如何以 **Token Buckets** 管理数据/算力预算、如何组合**对比/重建/策略蒸馏/频域**等多源损失并进行**梯度冲突调和**，以及如何在产出端设置**“可迁移性体检”**，确保后续 RL、仿真与 Sim‑to‑Real 可顺滑接力。读完本章，你应能：
>
> 1. 复现实用的两阶段预训练流水线；2) 将对齐目标与工程约束（带宽、时延、安全）合并进损失与日程；3) 用若干低成本 Probe 评估“是否具备 RL/Sim‑to‑Real 可迁移性”。

---

## 7.1 总览与阶段划分：模态 → 对齐 → 指令化

**目标**：将三态学习拆解为稳定、可迭代的阶段化流程，并明确每一阶段的**可观测指标**与**退出条件**。

```
[视觉表征 V]──┐
              ├─► 阶段Ⅱ：跨模态对齐（V–L, L–A, V–A）
[语言基座 L]──┘            │
                           ├─► 指令化与轻量监督（SFT/偏好）
[行动先验 A]───────────────┘            │
                                        └─► 模型级/智能体级 RL（见第8–11章）
```

**阶段Ⅰ（模态内）**：

* 视觉：对比/掩码建模/视频预测，产出**稳健、多尺度、时序一致**的表征。
* 语言：复用通用 LLM + 领域词表/Adapter/指令化语料，获得**可解释编排**能力。
* 行动：从示教/回放中预建**轨迹分布**与**控制先验**（加速度/跃度/带宽）。

**阶段Ⅱ（对齐）**：

* V–L：语义对齐 + 局部细节保真（短语-区域对应）。
* L–A：从“指令/目标”到“迹/控制”的映射与**时序一致性**。
* V–A：视觉约束下的行动可实现性与**频域耦合**（低带宽、安全）。

> 📌 **经验法则**：若“阶段Ⅰ”的表征在 OOD Probe 上**置信度过饱和**（过度自信），暂停“阶段Ⅱ”，优先做**温度缩放/能量分数校准**与**开集识别**再推进。

---

## 7.2 视觉预训练：CNN/对比/掩码视频自监督

**对比学习（CL/InfoNCE）**：
给定成对样本 ((x_i, y_i))（可为图–文、视–视增强），相似度 (s(\cdot,\cdot)) 与温度 (\tau)：

$$
\mathcal{L}_{\text{InfoNCE}}
= -\frac{1}{N}\sum_{i=1}^{N}
\log\frac{\exp\left(s(x_i,y_i)/\tau\right)}
{\sum_{j=1}^{N}\exp\left(s(x_i,y_j)/\tau\right)}.
$$

**掩码视频建模（MVM）与重建**：
对时空块集合 (\mathcal{M}) 预测像素/码本：

$$
\mathcal{L}_{\text{MVM}}=\sum_{(t,p)\in\mathcal{M}}
\left|\hat{x}_{t,p}-x_{t,p}\right|_1
\quad\text{或}\quad
\mathcal{L}_{\text{VQ-CE}}=-\sum \log P(z_{t,p}=z^\star_{t,p}).
$$

**时一致性与开集识别**：

* 一致性：短窗变换一致性 $\mathcal{L}*{\text{TC}}=\sum_t|f(x*{t+\Delta})-T_\Delta(f(x_t))|_2^2$。
* 开集：能量分数 $E(x)=-\tau \log \sum_k \exp(\phi_k(x)/\tau)$ 用于阈值化不确定性。

> 📌 **经验法则**：**不要**把 CL 负样本全当“真负样本”。对近邻帧/近景图像做**半负**或**软标签**可显著降低“伪负样本”带来的语义撕裂。

---

## 7.3 行动预训练：示教/频谱表征/控制先验

**目标**：让模型在无交互/无奖励下，学得**平滑、因果、可控**的轨迹分布。

* **行为克隆（BC）**：
  $$
  \mathcal{L}*{\text{BC}}=\mathbb{E}*{(s_t,a_t)}\big[-\log \pi_\theta(a_t\mid s_t)\big].
  $$

* **频域/带宽先验**（对轨迹 (x(t)) 的 DFT：(X[k])）：
  $$
  \mathcal{L}*{\text{spec}}=\sum*{k=0}^{K}w_k\big(,|X[k]|-|\hat{X}[k]|\big)^2,\quad
  w_k \propto \frac{1}{(1+k)^\alpha}.
  $$
  低频加权抑制高频抖动，映射到**舒适度**与**执行器带宽**。

* **几与动力学约束**（曲率 (\kappa)、加速度 (a)、跃度 (j)）：
  $$
  \mathcal{L}*{\text{dyn}}=\lambda*\kappa !\sum! [|\kappa|-\kappa_{\max}]*+ +
  \lambda_a !\sum! [|a|-a*{\max}]*+ +
  \lambda_j !\sum! [|j|-j*{\max}]_+ .
  $$

* **教师–学生蒸馏（控制教师 → 行动学生）**：用 MPC/CBF 的可行轨迹作为“软标签”，蒸馏到学生策略分布（KLD/EMD）。

> 📌 **经验法则**：若离线回放含**人类瞬时纠正**（高 jerk），用 **Huber** 或 **分位回归**替代 MSE，并在 (\mathcal{L}_{\text{spec}}) 中**下压高频权重**，再配合**jerk 裁剪**，能显著改善部署时的舒适度与执行器饱和。

---

## 7.4 语言预训练：复用通用 LLM 与领域适配

* **指令化与检索增强**：在领域语料上做轻量 SFT/LoRA/Adapter；对**任务规划/工具调用**相关模板做数据增强（问题分解、失败解释、动作约束表述）。
* **术语与单位一致性**：构建**控制/几何/安全**术语词表，统到语言端约束模板。
* **思考预算**：为后续 RFT/RL 做准备，引入**自反式评估**描述（“为什么此动作安全/可达”）。

> 📌 **经验法则**：领域适配尽量**冻结主干**仅训练适配层；当你发现对齐阶段 L–A 过拟合文本花样时，提高**术语模板比重**并降低“花式表达”的采样温度。

---

## 7.5 训练日程（Curriculum）：难度分级与混合采样

**目标**：让学习信号“由易到难、先稳后快”，避免早期梯度噪声破坏跨模态对齐。

* **难度维度**：视角变化、光照/噪声、遮挡率、目标密度、动作曲率、文本歧义度。
* **冻结/解冻策略**：

  * 早期冻结 LLM 主干 & 视觉低层，先学门控/交互层；
  * 中期逐步解冻视觉高层与跨模态注意力；
  * 后期小步解冻语言高层用于任务化表达。

### 7.5.1 日程原型示例（数字可按资源调整）

* **总步数**：(500\text{k})；**批大小**：按 GPU/TPU 资源。
* **阶段 A（0–100k）**：模态内（V:60%、A:25%、L:15%），除顶层外全部冻结跨模态层。
* **阶段 B（100k–300k）**：V–L 对齐 + 轻量 L–A（对比:40%、重建:25%、蒸馏:20%、频域:15%）。
* **阶段 C（300k–450k）**：三模态联合（门控/互信息/循环一致），逐步解冻跨模态注意力。
* **阶段 D（450k–500k）**：指令化微调（SFT 小步长）+ 稳定化（EMA/噪声退火）。

**ASCII 时间线**

```
步数:   0k         100k               300k            450k       500k
        |-----------|-------------------|---------------|----------|
阶段:   A  模态内   B  V–L+轻L–A        C  三模态联合    D  指令化/稳定
冻结:   L主干锁     解跨模态顶层        逐步解冻         小步全局解冻
损失:   重建/对比   +对齐/蒸馏/频域     +互信息/循环     +SFT/EMA
```

> 📌 **经验法则**：**不要**在阶段 A 就上强 L–A；先把视觉时序稳定住（错位鲁棒），否则对齐阶段会把**时域偏差**“语言化”，后续很难纠正。

---

## 7.6 Token Buckets 分配：按模态/任务/难度的预算治理

**设定**：为每个数据簇/任务 (k) 分配桶容量 (B_k)、漏出速率 (r_k)、当前已消耗令牌 (u_k(t))。每次采样满足：
$$
p_k(t) \propto \max\big(0,,B_k - u_k(t)\big)^\gamma \cdot q_k(\text{难度}, t),
$$
其中 (q_k) 可随课程升高难度，(\gamma) 控制“补齐性”。

**多目标公平**：加入下限 (p_k^{\min}) 防止冷门任务饿死；关键安全/开集 Probe 的桶给**优先级**。

**ASCII：三桶示意**

```
[V-CLIP桶] [L-指令桶] [A-轨迹桶]
   ████▊      ██▏        ████▍      ← 当前剩余令牌（越高越优先被抽样）
漏速 r:       r_v        r_l        r_a
```

> 📌 **经验法则**：当对齐损失震荡，先看**桶是否失衡**——常见现象是“容易样本”被过度采样导致**可见集过拟合**与**OOD 崩溃**。

---

## 7.7 对齐数据构建：配对、三元组、合成与清洗

* **配对/三元组**：((v,l,a))、((v,l))、((v,a)) 混合；引入**循环一致**约束：
  $$
  \mathcal{L}*{\text{cycle}}=|g*{L\to A}(g_{V\to L}(v))-g_{V\to A}(v)|_1 + \cdots
  $$
* **合成数据**：程序化生成**可控难度**样本（视角/光照/交通密度/目标曲率），便于课程与“失效放大”。
* **清洗**：

  * 时间戳对齐（相机–IMU–里程计），剔除**错位帧**；
  * 文本去幻觉/去冗词，保留**约束短语**（“不超速/不越线/保持时距”等）；
  * 行动轨迹去毛刺（jerk 裁剪、速度负值异常）。

> 📌 **经验法则**：优先做**时域对齐清洗**。时间错位 50–100 ms 就足以让 L–A 对齐学出**系统性滞后**，后续再做延迟补偿的代价更大。

---

## 7.8 损失与多目标优化：权重平衡与梯度冲突缓解

**总损失**（示意）：
$$
\mathcal{L}
= \lambda_{\text{CL}}\mathcal{L}_{\text{InfoNCE}}

* \lambda_{\text{REC}}(\mathcal{L}*{\text{MVM}}+\mathcal{L}*{\text{VQ-CE}})
* \lambda_{\text{KD}}\mathcal{L}_{\text{KD}}
* \lambda_{\text{spec}}\mathcal{L}_{\text{spec}}
* \lambda_{\text{dyn}}\mathcal{L}_{\text{dyn}}
* \lambda_{\text{cycle}}\mathcal{L}_{\text{cycle}}.
  ]

**自适应权重（不确定性加权）**：
$$
\mathcal{L}_{\text{multi}}=\sum_i \frac{1}{2\sigma_i^2}\mathcal{L}_i + \log \sigma_i,
$$
(\sigma_i) 可学习，反映任务噪声与重要度。

**梯度冲突调和**：

* **PCGrad**：若两任务梯度夹角为负，则投影去除相互冲突分量。
* **GradNorm**：按目标梯度范数平衡多任务学习进度。
* **损失退火**：早期放大稳定项（重建/频域），中后期提高对齐/蒸馏权重。

> 📌 **经验法则**：当你需要“快对齐、慢重建”，先**退火 InfoNCE 的温度 (\tau)** 而不是一味提高其权重。较大的 (\tau) 往往能缓解**过分拉开难负样本**导致的训练抖动。

---

## 7.9 正则与稳定：模态均衡、去塌缩、负迁移防护

* **模均衡 Dropout**：训练时随机屏蔽某一模态（按小概率），强制学习**跨模态冗余**。
* **去塌缩**：对齐空间增加**分布约束**（如高斯混合先验、最大化对角协方差），防止所有样本聚到单团。
* **负迁移防护**：对“视觉噪声段/文本冗词段/异常轨迹段”设置**遮挡标记**，避免错误传播到对齐头。
* **动量教师（EMA）**：跨模态编码器用动量更新的教师网络做目标，平滑训练信号。

> 📌 **经验法则**：一旦发现“**好看但不可行**”的动作（轨迹平滑却越界），立刻提高 (\lambda_{\text{dyn}}) 并引入**QP 投影后的教师轨迹**做蒸馏；不要指望后续 RL 全部修复。

---

## 7.10 可迁移性检查点：面向 RL/仿真/Sim‑to‑Real 的“可迁移性体检”

**目的**：在预训练末期进行**体检**，只保留“对后续强化学习友好”的检查点。

**体检项目**（建议至少通过 8/10）：

1. **延迟鲁棒 Probe**：输入时间错位 (\pm)50–100 ms，L–A 输出相位偏差 < 阈值。
2. **频域平滑度**：(\int |j(t)| dt) 与高频能量比不超过教师上界的 1.2×。
3. **开集拒识**：未知场景能量分数分离度（AUROC）≥ 0.9。
4. **循环一致**：V→L→A 与 V→A 的 L1 距离 < 设定阈值。
5. **文本约束遵循**：加入否定/边界词（“不越线/不逆行”）的遵循率 ≥ 0.98。
6. **对抗随机化**：光照/噪声/遮挡 stress 下的对齐召回率下降 < 20%。
7. **低带宽可执行性**：将动作下采样至执行器带宽，误差 < 设定阈值。
8. **教师一致性**：MPC/CBF 参考下，KLD(学生‖教师) < 预设上限。
9. **负样本诡计**：近邻帧伪负样本测试下的 InfoNCE 退化 < 10%。
10. **数据泄露扫描**：仿真评测集与训练集视/语/轨迹去重通过。

**导出物**：

* **权重包 + 日志**（包含温度、桶状态、损失曲线）；
* **Probe 报告**（包含时域/频域指标与门限）；
* **兼容第8章**的 RFT/RL 初始化配置（冻结/学习率建议）。

> 📌 **经验法则**：当**体检得分相近**时，优先选**高不确定性识别能力**而非**表面成功率更高**的检查点；后者在 Sim‑to‑Real 常因过度自信而失稳。

---

## 本章小结

* 采用**两阶段**策略：**模态内**稳固表征 → **跨模态对齐**构建 V–L–A 桥梁。
* 用 **Curriculum + Token Buckets** 管理采样，兼顾难度爬升与数据/算力公平。
* 将 **对比/重建/蒸馏/频域/动力学**等损失在**自适应权重**与**梯度外科**下稳健融合。
* 在产出端实施 **“可迁移性体检”**，以**时域、频域、开集、可执行性**四维指标为主，筛选可用于 RL 与 Sim‑to‑Real 的检查点。

---

## 常见陷阱与错误（Gotchas）与调试技巧

1. **伪负样本导致的对齐撕裂**

   * 现象：近景/相邻帧被强行拉远，跨模态嵌入空间不稳定。
   * 对策：软负样本、近邻抑制、温 (\tau) 退火、语义聚类内不做负样本。

2. **时间错位与滞后学习**

   * 现象：L–A 输出整体滞后，RL 阶段需要过大延迟补偿。
   * 对策：在预训练加入**错位增强**与**延迟校正 Probe**；曲线拟合时加入**相位惩罚**。

3. **频域失衡：表面平滑、实则越界**

   * 现象：低频平滑但轨迹越线/超速。
   * 对策：增加 (\mathcal{L}_{\text{dyn}}) 与 QP 投影蒸馏；限制频域损失对低频的过度偏好。

4. **桶失衡导致的 OOD 崩溃**

   * 现象：对齐在训练分布上优雅，OOD 上断崖。
   * 对策：检查 Token Buckets 与课程采样，设定 (p_k^{\min}) 与“失效放大”样本的高优先级。

5. **负迁移：噪声段污染对齐头**

   * 现象：少量坏数据牵引模型到错误对齐。
   * 对策：显式**遮挡/异常标记**、段级权重衰减、EMA 教师平滑目标。

6. **损失权重拍脑袋**

   * 现象：调参靠感觉、震荡频繁。
   * 对策：使**不确定性加权**与**GradNorm/PCGrad**；记录每项梯度范数趋势并做闭环调整。

7. **语言端“花式表达”压过“约束短语”**

   * 现象：模型更会“说”，却不懂“规”。
   * 对策：提高“否定/边界”模板占比；降低 SFT 阶段的采样温度与 n‑gram 多样性奖励。

8. **检查点选择只看 val loss**

   * 现象：线上 RL/仿真效果与离线指标脱节。
   * 对策：采用本章的**体检十项**，尤其关注**开集识别**与**低带宽可执行性**。

9. **忽视执行器带宽与量化误差**

   * 现象：实验室顺滑，设备端抖动。
   * 对策：预训练即引入**带宽约束**与**量化噪声注入**，在 (\mathcal{L}_{\text{spec}}) 中设定截止频率。

10. **把“预训练”当终点**

    * 现象：期望预训练直接可部署。
    * 对策：把预训练视为**证据链**的前半程；为第8–11章的 RFT/RL/Sim‑to‑Real 留出**结构化接口**（残差策略、RTA 兼容）。

---

> **本章产出清单（建议）**
>
> * 训练日程 JSON（阶段边界/冻结策略/损失退火曲线）。
> * Token Buckets 配置与每桶消费曲线。
> * 多目标损失与梯度统计面板（范数、夹角、PCGrad 命中率）。
> * “可迁移性体检”报告（含阈值、通过/失败项与建议）。

> **衔接提示**：带着通过体检的检查点进入第8章（模型级 RL 微调/RFT），优先采用**小步长、行为正则与 KL 约束**，以保持本章形成的**平滑/可执行**行动分布与**可解释**对齐结构。

