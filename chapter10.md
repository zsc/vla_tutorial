（交流可以用英文，所有文档中文，保留这句）

# 第10章 多智能体博弈与协调：从均衡理论与 MARL 到工程落地

> **开篇段落（本章导引）**
> 多智能体问题的本质是**相互耦合的决策与共享约束**。在自动驾驶无信号交汇、编队机器人、仓储搬运等场景中，**安全—效率—公平—舒适—社交合规**往往相互牵制。本章从**均衡理论（Nash/相关/Stackelberg/贝叶斯）**与**多智能体强化学习（MARL）**双线并行，落到**形式化安全屏蔽（LTL/STL/CBF）、求解器（MPC/MIQP/QP）**与**运行时保障（RTA）**的工程组合，给出**可复现的评测协议**与**落地模板**。读者将获得：
>
> 1. 何时用**均衡建模**，何时用**MARL 残差**；2) 如何将**安全约束**嵌入学习闭环；3) 面向**Sim‑to‑Real**的**通信与意图协议、对手失范处置和公平性**治理。

---

## 10.1 为什么是“多智能体”：外部性、互惠与礼让

* **式化**：标准静态博弈
  [
  G=\langle \mathcal N,{\mathcal A_i}*{i\in\mathcal N},{u_i}*{i\in\mathcal N}\rangle,\quad
  \pi_i\in\Delta(\mathcal A_i)
  ]
  其中 (\mathcal N) 为智能体集合，(\mathcal A_i) 为动作集，(u_i) 为效用/回报。
* **外部性**：一个体的加速/刹车改变他人的可行域与风险（如 TTC、可达集）。
* **礼让与社会合规**：策略含**他人体验项**（他人急刹/大跃度视为负外部性）。

> **经验法则 10‑A（问题刻画）**
> 先画出**共享约束**（碰撞避免、优先权、速度/加速度/跃度、CBF 不变集），再讨论个体目标；共享约束优先级**高于**个体效用。

---

## 10.2 均衡建模：Nash/相关均衡/Stackelberg 与效率—公平

* **Nash 均衡（NE）**：(\pi^*) 使得任一体单独偏离非增益
  [
  \forall i,\ \pi_i^*\in\arg\max_{\pi_i}\ \mathbb E_{a\sim \pi_i,\pi_{-i}^*}[u_i(a)].
  ]
* **相关均衡（CE）**：存在联合分布 (q(a)) 满足对任意 (i,a_i,a_i'),
  [
  \sum_{a_{-i}} q(a_i,a_{-i})\big(u_i(a_i,a_{-i})-u_i(a_i',a_{-i})\big)\ge 0.
  ]
  **解释**：有一个“信号器”给出相关建议，遵循建议无激励偏离。
* **Stackelberg（领从）**：Leader 承诺 (\pi_L)，Follower 取最佳应对 (\mathrm{BR}(\pi_L))。
* **效率与公平**：社会福利 (W=\sum_i w_i u_i)；**Anarchy 代价**（PoA）与**稳定性代价**（PoS）评估 NE 偏离最优的程度。
* **近似 NE（(\epsilon)-NE）**：**Nash gap**
  [
  \mathrm{Gap}(a)=\max_i\ \max_{a_i'}\big(u_i(a_i',a_{-i})-u_i(a)\big)\le \epsilon.
  ]

```
效率 ↑
      ┌────────────────  社会最优
      │     ●
      │            ○   NE
      │
      └────────────────────────→ 公平（差异↓）
```

> **经验法则 10‑B（范式选型）**
>
> * **NE/CE**：需要**解释性/合规性**强，通信可用（CE 时）——适合**策略审计**与合规证明。
> * **Stackelberg**存在**路权/优先权层级**（如应急车）或**基础设施先手**（路侧单元）。
> * **MARL**：动态、部分可观、非线性约束强时用作**近似与残差**，再配**安全投影**。

---

## 10.3 不完全信息与贝叶斯博弈：类型、信念与风险态度

* **贝叶斯博弈**：每个体有类型 (\theta_i\sim p(\theta))，效用 (u_i(a,\theta))。目标最大化**条件期望效用**。
* **风险敏感**：CVaR(*\alpha) 或极端事件加权
  [
  J_i(\pi)=\mathrm{CVaR}*\alpha\big(\sum_t r_{i,t}\big)
  ]
  兼顾长尾安全。
* **鲁棒博弈**：对不确定集 (\Theta) 取 (\min_{\theta\in\Theta}) 的 worst‑case 效用。

> **经验法则 10‑C（类型与意图）**
> 把“激进/保守、顺/逆规、自动/人类驾驶、网联/非网联”**折叠为类型变量**，在规划器中进行**信念更新**与**保守化半径**调节。

---

## 10.4 学习与收敛：虚拟对弈、无悔学习与复制子动态

* **无悔学习**：平均外悔 (\bar R_T\to 0) 则经验分布收敛到**粗相关均衡（CCE）**。
  [
  \bar R_T=\max_{a_i'}\frac1T\sum_{t=1}^T \big(u_i(a_i',a_{-i}^t)-u_i(a_i^t)\big)\to 0.
  ]
* **虚拟对弈（Fictitious Play）**：对他人频率做最优响应，潜在博弈中收敛。
* **复制子动态**（连续时间）：
  [
  \dot\pi_i(a)=\pi_i(a)\big(u_i(a,\pi_{-i})-\langle \pi_i,u_i(\cdot,\pi_{-i})\rangle\big).
  ]

> **经验法则 10‑D（可收敛性）**
> 尽量将交互建模为**潜在博弈**或引入**潜在函数近似**；否则转向**CCE**并在工程上提供**屏蔽**与**恢复**路径。

---

## 10.5 MARL 综述：CTDE、价值分解、策梯度、对手建模与通信

* **CTDE**（集中训练、分散执行）：训练用全局状态 (s)/联合动作 (a)，执行仅用局部观测 (o_i)。
  [
  \nabla_{\theta_i}J=\mathbb E\big[\nabla_{\theta_i}\log\pi_i(a_i|o_i),A_i^{\text{central}}(s,a)\big].
  ]
* **价值分解**：VDN (Q_{tot}=\sum_i Q_i)；QMIX 要求单调性 (\partial Q_{tot}/\partial Q_i\ge 0)。
* **多智能体策梯度**：MADDPG/MAPPO；**信用分配**（COMA 优势）
  [
  A_i=Q(s,a)-\sum_{a_i'}\pi_i(a_i'|o_i),Q(s,(a_i',a_{-i})).
  ]
* **对手建模与通信**：显式 belief 网络、消息通道（bandwidth/延迟/安全）。

> **经验法则 10‑E（训练稳定性）**
> 非平稳性由**对手策略漂移**引起：使用**对手池/策略混合（PSRO/BR 缓存）**、**冻结‑解冻节奏**与**KL/行为正则**降低漂移。

---

## 10.6 约束与安全：CMDP、拉格朗日/原始–对偶、鲁棒与 RTA

* **约束 MDP**：(\max_\pi \mathbb E[\sum\gamma^t r_t]) s.t. (\mathbb E[\sum\gamma^t c_t]\le d)。
  **拉格朗日**：
  [
  \max_\pi\min_{\lambda\ge0}\ \mathbb E\textstyle\sum_t\gamma^t\big(r_t-\lambda c_t\big)+\lambda d.
  ]
  更新 (\lambda\leftarrow\big[\lambda+\eta(\hat C-d)\big]_+)。
* **控制障碍函数（CBF）**：安全集 (\mathcal S={x:h(x)\ge0})，满足
  [
  \dot h(x)+\alpha h(x)\ge0
  ]
  或离散式 (h(x_{t+1})-h(x_t)+\alpha h(x_t)\ge0)。
  **QP 投影**（安全屏蔽）：
  [
  \min_u |u-u_{\text{nom}}|^2\ \ \text{s.t.}\ \ \nabla h f(x)+\nabla h g(x)u+\alpha h(x)\ge0.
  ]
* **运行时保障（RTA）**：在线监控预测风险，违反阈值时切换到**保证安全的备份控制**。

```
Teacher (MPC/博弈) + Residual MARL
           │
           ▼
   QP/CBF Safety Projection  ──►  RTA Gate  ──►  执行器
```

> **经验法则 10‑F（两层盾）**
> 先用**QP/CBF**做**连续域安全投影**，再用**RTA**处理**不可预见离散事件**（对手闯入、传感失配）。

---

## 10.7 形式化方法与求解器：LTL/STL Shield、CBF/CLF、(M/I)QP

* **时序逻辑**：将“永不碰撞、最终通过、红灯必停”等映射为 LTL/STL 公式，在线/离线合成**盾（Shield）**。
* **MPC with CBF**：在滚动优化中把 CBF 作为硬约束/软约束。
* **MIQP**：离散决策（让/行、道次、车道变换）+ 连续控制（速度/转角）混合，big‑M 约束编码优先权。

> **经法则 10‑G（可审计性）**
> 将**合规红线**用 STL/LTL/CBF 显式化；训练/推理日志**记录约束活性度**与**QP 残差**，供审计与回放。

---

## 10.8 案例：**无信号交汇**协同通行（“博弈 + 求解器 + 残差”）

**几何与交互**

```
        ↑  B
--------┼---------
        │
A → → → + ← ← ← D
--------┼---------
        │
        ↓  C
```

* **共享约束**：时空冲突区、最小时距 (\mathrm{TTC}=\frac{d}{\max(\epsilon,v_{\text{rel}})}\ge \tau)、最大加/减速度 (|a|\le a_{\max})、跃度 (|\dot a|\le j_{\max})。
* **建模**：

  1. **贝叶斯类型**（激进/保守）；
  2. **Stackelberg**（主路/支路或路侧单元作 Leader）；
  3. **CE**（车群有轻通信时基于信号器相关化）。
* **落地流水线**

  1. **教师（MPC/博弈求解）**：在 CBF/STL 约束下求社会目标（效率+公平+舒适）解；
  2. **学生（MARL 残差）**：在教师解的**残差空间**学习人性化/细节；
  3. **QP 投影**：将残差叠加后的控制量投影回安全集；
  4. **RTA**：监测 (\mathrm{TTC},\ \text{CBF margin},\ \text{jerk}) 等阈值，越界即切换备份；
  5. **僵局解除**：注入轻量随机/优先权令牌，或路侧广播**turn‑taking**。

> **经验法则 10‑H（礼让定价）**
> 引入**礼让成本**：导致他车 (|a|>a_c) 或 (|\dot a|>j_c) 记负分；在社会福利中加入**(\alpha)-公平**项（见 10.13），兼顾效率与公平。

---

## 10.9 评测协议：安全/效率/舒适/社交合规/公平性的多目标

* **安全**：碰撞率↓、最小 (\mathrm{TTC}) 分位数↑、越界率（CBF 违约）↓。
* **效率**：通行率、平均延误、ETA 偏差。
* **舒适**：加速度/跃度 RMS、急刹频次。
* **社交合规**：红线违例 0、让行礼让率、对交通流扰动。
* **公平**：等待时间 Gini 系数、**(\alpha)-公平**（10.13）、**Nash gap** 作为稳定性指标。
* **覆盖率报告**：参数场景（来车速度/间隙/能见度）网格覆盖 + **长尾簇**抽样。
* **异常注入**：失范/恶意对手占比 (p)，评估**可恢复性**与**优雅降级**。

---

## 10.10 工程设计模式：分层协同、消息/意图、对手建模与失效回放

* **分层**：预测（他车意图/类型 belief）→ 规划（博弈/MPC+残差）→ 控制（QP/CBF）→ 屏蔽与 RTA。
* **消息与意图**：**显式** turn‑taking/路权令牌、**隐式**轨迹意图（闪灯/微操）。
* **对手建模**：策略池/混合分布、类型滤波、最坏情形半径。
* **运维**：统一时间戳、可复现随机种子、回放与“影子模式”AB 实验、失败簇成因标注。

---

## 10.11 从多智能体仿真到真实部署：域随机化、隐域估计与策略残差

* **域随机化**（交互域）：对人类驾驶**反应时/保守度**、通信丢包/延迟进行课程化随机。
* **隐域估计**：在线估计对手类型 (\hat\theta)，将策略写成 (\pi(a|o,\hat\theta))。
* **残差策略**：(,u=u_{\text{teacher}}+\Delta u_{\text{MARL}})，残差幅度受**RTA 限幅**。

---

## 10.12 通信与意图协议：显式 turn‑taking / implicit signaling

* **显式**：最小消息集（意图、预计到达时间窗、优先权令牌），拥塞时退化为**先到先服务**。
* **隐式**：轨迹意图编码（减速坡度、车头姿态）；对人类友好**可解释提示**（转向灯/刹灯节拍）。
* **健壮性**：消息签名/时效、丢包下回退为 CE/BR 策略。

---

## 10.13 公平性度量：价格‑公平权衡、社交合规罚则

* **(\alpha)-公平**：对“资源”(x_i)（如通过率/绿窗）
  [
  U_\alpha(x)=
  \begin{cases}
  \sum_i \frac{x_i^{1-\alpha}}{1-\alpha}, & \alpha\neq 1[4pt]
  \sum_i \log x_i, & \alpha=1
  \end{cases}
  ]
  (\alpha\uparrow) 更公平、效率可能下降。
* **礼让罚则**：若我方动作使他车 (|a|>a_c) 或 (|\dot a|>j_c)，施加惩罚 (\lambda_c,\lambda_j)。
* **队列公平**：等待时长 Gini/95%分位差、最大滞后界。

---

## 10.14 对手失范/恶意行为：异常 agent 注入与恢复流程

* **检测**：**Nash gap/CE 条件**异常、**CBF margin**持续为负、**TTC** 急剧下降。
* **处置**：策略转入**保守集**（更大安全半径），路侧触发**turn‑taking**重置；记录证据。
* **恢复**：异常消退后**缓启动**（jerk 限制、渐近恢复权重）。

---

## 10.15 小结与与第 11 章（Sim‑to‑Real）的接口

* **两条路并行**：**均衡可解释性** + **MARL 可扩展性** → **残差结构化**。
* **两层安全**：**QP/CBF 投影** + **RTA 切换**。
* **面向 S2R**：交互域随机化、意图协议与**异常回放基准**。下一章将把本章策略接上**系统辨识/传感延迟/带宽配平**等现实要素，完成**准入清单与验收**。

---

## 本章小结（Key Takeaways）

1. **建模优先**：先固化**共享安全约束**与**红线逻辑**，再优化个体目标。
2. **均衡 vs. 学习**：NE/CE/Stackelberg 提供**审计友好**骨架，MARL 提供**非线性与人性化残差**。
3. **安全闭环**：CMDP‑拉格朗日 + CBF‑QP + RTA，保证**硬约束不被学习突破**。
4. **评测多目标**：安全、效率、舒适、合规、**公平**与**稳定性（Nash gap/PoA）**共同报告。
5. **实战路线**：教师（博弈/MPC）→ 学生（MARL 残差）→ 屏蔽（QP/CBF）→ 保障（RTA）→ S2R。

---

## 常见陷阱与错误（Gotchas）与调试技巧

* **非平稳性被忽视**：把环境当作静态 MDP 训练，导致部署时崩溃。
  *调试*：对手策略池与混合采样；冻结‑解冻与 KL 正则；离线回放 + 在线微调 A/B。
* **把“安全”当软目标**：将碰撞当 soft penalty，训练中短期收益压过安全。
  *调试*：CBF/QP 作为**硬约束**，RTA 兜底；分离“安全红线”与“性能目标”优化器。
* **信用分配不当**：个体奖励受噪声淹没。
  *调试*：COMA/差分奖励（difference rewards）、基于影响度的 credit。
* **均衡选型错误**：明明有路权层级却用 NE，或通信稀缺却假设 CE。
  *调试*：做**协议可用性审计**；Stackelberg/CE 需**最小通信保证**。
* **过拟合自博弈**：只在自我同质对手中训练，现实混杂类型失败。
  *调试*：类型随机化（激进/保守/违规）、对手分布移位测试（dataset/agent shift）。
* **MIQP 不稳定或求解超时**：混合整数求解卡住实时性。
  *调试*：预生成策略库/热启动；将离散决策下沉为**RTA/令牌**；对连续层用 QP。
* **通信脆弱**：把轻通信当可靠总线。
  *调试*：显式**丢包/延迟/篡改**随机化；设计**无通信回退**（CE/BR 备份）。
* **公平性缺位**：整体吞吐高但个别车辆长期饥饿。
  *调试*：(\alpha)-公平/等待时间 Gini 进入目标；监控**最坏分位**。
* **评测窄化**：只报平均成功率，忽略长尾与边界条件。
  *调试*：**参化场景网格 + 长尾聚类**覆盖；异常注入（恶意/失范）与恢复指标。
* **日志不可审计**：缺失约束活性度、QP 残差与切换原因。
  *调试*：强制记录**CBF margin、RTA 触发、Nash gap**与“谁先行”的证据链。

> **排障清单（最小闭环）**
>
> 1. CBF margin 全时 ≥0？ 2) (\mathrm{TTC}) 分位数合规？ 3) RTA 触发频率 < 1% 吗？
> 2. Nash gap ≤ (\epsilon)？ 5) 等待时间 Gini ≤ 阈值？ 6) 影子模式与回放一致？

---

### 附：常用公式与度量速览

* **TTC**：(\mathrm{TTC}=\frac{d}{\max(\epsilon,v_{\text{rel}})})。
* **Nash gap**：(\max_i\max_{a_i'} [u_i(a_i',a_{-i})-u_i(a)])。
* **(\alpha)-公平**：见 10.13。
* **CMDP 拉格朗日更新**：(\lambda\leftarrow[\lambda+\eta(\hat C-d)]_+)。
* **CBF QP**：见 10.6。
* **COMA 优势**：见 10.5。

> **本章产出（建议）**
>
> * 一页纸**设计蓝图**（教师‑学生‑屏蔽‑RTA 数据流）。
> * **评测卡**：指标、阈值、异常注入脚本清单与场景覆盖率报告模板。
> * **回放用例包**：含代表性僵局/失范/公平性边界场景。

