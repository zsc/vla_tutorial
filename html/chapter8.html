<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第8章 强化学习与微调：从指令遵循到策略优化</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Visual-Language Action Model: 预训练、MARL 和 Sim-to-Real</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章 导论与动机案例</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章 视觉模态：从像素到可行动的表征</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章 语言模态：符号推理、过程编排与系统调度</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章 行动模态：从信号处理到鲁棒控制</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章 模态对齐（Vision–Language–Action）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章 隐式 3D 时空结构的引入</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章 预训练：模态预训练与跨模态对齐</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章 强化学习与微调：从指令遵循到策略优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章 基于仿真的智能体级强化学习（单智能体）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章 多智能体博弈与协调：从均衡理论与 MARL 到工程落地</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章 神经化 Sim-to-Real：弥合仿真与现实的认知与动力学鸿沟</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="8">第8章 强化学习与微调：从指令遵循到策略优化</h1>
<h2 id="81">8.1 开篇段落</h2>
<p>在前面的章节中，我们已经构建了一个强大的 VLA 预训练基座模型，它能够通过模仿学习（如行为克隆）对世界产生初步的理解和行动能力。然而，仅仅“模仿”是不够的。真实世界的任务要求智能体能够<strong>优化</strong>性能、适应未见过的场景、并从交互的后果中学习，而不仅仅是复制专家数据。本章将深入探讨如何通过强化学习（RL）和相关微调技术，将我们的 VLA 模型从一个被动的“模仿者”转变为一个主动的“决策者”。我们将系统地比较监督微调（SFT）、基于反馈的微调（RFT）和经典强化学习的优劣与适用场景，并介绍策略优化的核心算法、自反式指导等前沿技术。特别地，我们将借鉴 <strong>Seed1.5-Thinking</strong> 等前沿工作中的经验，深入探讨在处理需要复杂推理（如长思维链 CoT）的任务时，如何设计数据、奖励模型和算法来克服 RL 训练的<strong>不稳定性</strong>。本章的学习目标是，掌握一套将预训练模型转化为能够进行目标导向优化、同时兼顾安全与效率的高级策略的完整方法论，并学会使用离线策略评估（OPE）在部署前科学地评估其潜在性能，为后续的仿真与 Sim-to-Real 迁移奠定坚实的策略基础。</p>
<hr />
<h2 id="82">8.2 文字论述</h2>
<h3 id="821-sft-vs-rft-vs-rl">8.2.1 SFT vs. RFT vs. RL：记忆与泛化的权衡</h3>
<p>从预训练基座模型出发，提升其特定任务能力的路径并非只有一条。选择哪条路径，取决于数据可用性、任务复杂度和对泛化能力的要。</p>
<ul>
<li>
<p><strong>监督微调 (Supervised Fine-Tuning, SFT)</strong></p>
<ul>
<li>
<p><strong>核心思想</strong>: SFT 本质上是<strong>行为克隆 (Behavior Cloning, BC)</strong> 的延续。它使用高质量的“状态-行动”对 <code>(s, a)</code> 数据集，通过最大化专家行动的对数似然来进行微调。
    $$
L_{SFT}(\theta) = -\mathbb{E}_{(s, a) \sim \mathcal{D}_{expert}} [\log \pi_\theta(a|s)]
$$</p>
</li>
<li>
<p><strong>代表性工作</strong>: 几乎所有基于模仿学习的机器人和自动驾驶模型，如早期的 <strong>ALVINN</strong> 到现代的 <strong>Gato</strong> 和 <strong>RT-1</strong>，都以 SFT/BC 作为其核心训练范式或初始阶段。<strong>Seed1.5-Thinking</strong> 等工作也明确指出，从一个经过 SFT 的模型开始 RL，相比直接从基座模型开始，能够产出更可读的输出、减少幻觉，为后续的 RL 阶段提供一个坚实的基础。</p>
</li>
<li><strong>优点</strong>: 简单、稳定、对数据质量要求高但利用直接。是冷启动一个策略的最佳起点。</li>
<li><strong>缺点</strong>:<ol>
<li><strong>分布偏移 (Distribution Shift)</strong>: 模型在训练分布上表现良好，但一旦在执行中遇到一个微小的、未见过的状态，其后续的轨迹可能会迅速偏离专家数据覆盖的范围，导致错误累积。</li>
<li><strong>因果混淆 (Causal Confusion)</strong>: 模型可能学习到虚假的关联而非真正的因果关系。</li>
<li><strong>次优天花板</strong>: 模型的性能上限被数据集中最好的专家所限制，无法发现超越专家的更优策略。</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>基于反馈的微调 (Reinforcement Fine-Tuning, RFT)</strong></p>
<ul>
<li>
<p><strong>核心思想</strong>: 此类方法，以 <strong>RLHF (Reinforcement Learning from Human Feedback)</strong> 和 <strong>RLAIF (RL from AI Feedback)</strong> 为代表，承认了为复杂任务设计一个精确、稠密的奖励函数极其困难。人类（或更强的 AI 模型）更擅长做<strong>比较判断</strong>。其流程通常为：</p>
<ol>
<li>从当前策略中采样若干个轨迹或行动序列。</li>
<li>让人类或 AI 标注者对这些序列进行偏好排序。</li>
<li>
<p>利用这些偏好数据训一个<strong>奖励模型 (Reward Model, RM)</strong>，$r_\phi(s, a)$，使其能够预测人类的偏好。通常使用 Bradley-Terry 模型，其损失函数为：
    $$
L(\phi) = -\mathbb{E}_{(s, a_w, a_l) \sim \mathcal{D}_{pref}} [\log(\sigma(r_\phi(s, a_w) - r_\phi(s, a_l)))]
$$
    其中 $a_w$ 是更优的行动 (winner)，$a_l$ 是较差的行动 (loser)。</p>
</li>
<li>
<p>使用强化学习算法（如 PPO）优化策略 $\pi_\theta$，以最大化来自奖励模型 $r_\phi$ 的期望回报。</p>
<ul>
<li><strong>代表性工作</strong>: <strong>InstructGPT</strong> 和 <strong>ChatGPT</strong> 在语言模型领域的成功，将 RLHF 推向了主流。在 VLA 领域，这被用于对齐如“驾驶舒适度”、“操作灵巧性”等主观指标。</li>
<li><strong>优点</strong>: 能够对齐模糊、主观的人类价值观，绕过了手动设计奖励函数的难题。</li>
<li><strong>缺点</strong>: 奖励模型本身可能存在偏差或被“黑客攻击”(Reward Hacking)，并且需要持续的人工或 AI 标注，成本高。</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>环境交互强化学习 (RL from Environment Interaction)</strong></p>
<ul>
<li><strong>核心思想</strong>: 智能体直接与环境（通常是仿真器）交互，接收环境反馈的标量奖励信号 $r(s, a)$，并以此优化策略以最大化累积回报 $G_t = \sum_{k=0}^\infty \gamma^k r_{t+k+1}$。</li>
<li><strong>优点</strong>: 潜力无限，能够发现超越人类的、全新的解决方案。</li>
<li><strong>缺点</strong>: <strong>样本效率极低</strong>，尤其是在真实世界中。奖励函数设计是关键难点，且在线探索可能带来安全风险。</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Rule-of-Thumb</strong>:</p>
<ol>
<li><strong>启动</strong>: 永远从 SFT/BC 开始，获得一个可用的基础策略。</li>
<li><strong>对齐</strong>: 当任务目标难以用数值量化（如“优雅地”并线）时，转向 RFT/RLHF。</li>
<li><strong>极限优化</strong>: 当你拥有一个高保真仿真器和明确的性能指标（如通行时间、能耗）时，使用经典 RL 进行极限压榨。</li>
<li><strong>混合使用</strong>: 最佳实践通常是三者的结合：用 SFT 初始化，用 RLHF 对齐复偏好，同时用环境稀疏奖励（如任务成功）作为补充，并始终用 BC loss 进行正则，防止策略“忘记”基本技能。</li>
</ol>
</blockquote>
<h3 id="822">8.2.2 策略优化：核心算法与正则化</h3>
<ul>
<li>
<p><strong>PPO (Proximal Policy Optimization)</strong>: 这是当前 RL 领域的“瑞士军刀”。PPO 通过一个<strong>截断的替代目标函数 (Clipped Surrogate Objective)</strong> 来限制每次更新的步长。
    $$
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t) \right]
$$
    其中，$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ 是重要性采样权重，$\hat{A}_t$ 是优势函数估计。</p>
</li>
<li>
<p><strong>离线强化学习 (Offline RL)</strong>: 如何直接利用海量的离线数据集进行 RL 而无需昂贵的在线交互，是离线 RL 的核心问题。</p>
<ul>
<li><strong>代表性工作</strong>:<ul>
<li><strong>CQL (Conservative Q-Learning)</strong>: 在 Q 函数的学习目标上增加一个正则项，惩罚在数据分布外的 <code>(s, a)</code> 对的高 Q 值。</li>
<li><strong>IQL (Implicit Q-Learning)</strong>: 通过期望回归，隐式地学习一个 Q 函数，避免了对 OOD <code>(s, a)</code> 的显式查询。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="823-rl-seed15-thinking">8.2.3 稳定化RL：来自 Seed1.5-Thinking 的前沿实践</h3>
<p>VLA 模型中的语言模块在执行复杂任务时，需要生成长的思维链（CoT）或行动计划。对此类长序列任务进行 RL 训练是<strong>高度不稳定</strong>的。<strong>Seed1.5-Thinking</strong> 等工作系统性地解决了这个问题，其核心思想可以归纳为一套组合拳，对我们极具借鉴意义：</p>
<ol>
<li>
<p><strong>价值函数预训练 (Value Pre-training)</strong>: 在开始 PPO 训练前，固定 SFT 策略，采样轨迹并使用蒙特卡洛回报来预训练价值模型。这确保了初始的价值函数与初始策略高度对齐，为后续的优势估计提供一个稳定、准确的起点，是防止早期训练崩溃的关键一步。</p>
</li>
<li>
<p><strong>精细化的优势估计 (Refined Advantage Estimation)</strong>:</p>
<ul>
<li><strong>解耦 GAE (Decoupled-GAE)</strong>: 为价值函更新和策略函数更新使用不同的 GAE 参数 $\lambda$（例如，$\lambda_{value}=1.0$ 做无偏更新，$\lambda_{policy}=0.95$ 来降低方差）。这使得价值和策略的优化目标可以解耦，更灵活地平衡各自的偏置-方差权衡。</li>
<li><strong>长度自适应 GAE (Length-adaptive GAE)</strong>: 将 $\lambda$ 设置为与响应长度 $l$ 相关的函数，如 $\lambda_{policy} = 1 - \frac{\alpha}{l}$。这使得长序列的 TD 误差分布更均匀，有效处理不同长度的 CoT。</li>
</ul>
</li>
<li>
<p><strong>改进的 PPO 裁剪 (Improved PPO Clipping)</strong>:</p>
<ul>
<li><strong>非对称裁剪 (Clip-Higher)</strong>: PPO 的标准对称裁剪可能过度抑制对低概率、但可能是正确探索方向的 token 的学习。通过使用非对称的裁剪边界，可以给予“好”的 token 更大的更新空间。
    $$
L^{CLIP-H}(\theta) = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon_{low}, 1+\epsilon_{high})\hat{A}_t) \right]
$$
    其中 $\epsilon_{high} &gt; \epsilon_{low}$，这鼓励模型探索新的、有益的行动。</li>
</ul>
</li>
<li>
<p><strong>混合损失函数 (Hybrid Loss Function)</strong>:</p>
<ul>
<li><strong>正样本语言模型损失 (Positive Example LM Loss)</strong>: RL 训练容易导致语言流畅性下降（所谓的“语言漂移”）。一个有效的对抗方法是，在 PPO 损失之外，对那些被判定为“正样本”（例如，成功完成任务或获得高奖励）的轨迹，额外施加一个标准的监督学习损失（NLL Loss）。
    $$
L(\theta) = L_{PPO}(\theta) + \mu \cdot L_{NLL}(\theta) \quad \text{for positive examples}
$$
这有助于模型在探索新策略的同时，不遗忘从 SFT 阶段学到的优秀语言模式和基本技能。</li>
</ul>
</li>
</ol>
<h3 id="824">8.2.4 自反式指导与奖励设计</h3>
<ul>
<li>
<p><strong>自反式指导</strong>: 大型 VLA 模型，特别是融合了强大语言能力的模型，具备了进行“自我反思”和“自我提升”的潜力。模型执行一个任务后，可以根据结果（成功、失败、或人类反）进行自我反思，生成“失败-原因-修正”的文本，并将其作为新的训练数据，形成一个持续改进的闭环。</p>
</li>
<li>
<p><strong>奖励设计：可验证 vs. 不可验证</strong>:</p>
<ul>
<li><strong>可验证问题 (Verifiable Problems)</strong>: 对于有明确正确答案的任务（如数学计算、代码生成、闭环控制中的目标达成），我们可以设计<strong>自动化验证器 (Verifier)</strong> 来提供准确、二元的奖励信号（成功/失败）。<strong>Seed1.5-Thinking</strong> 甚至更进一步，训练了一个 <strong>“会思考”的验证器 (Thinking Verifier)</strong>，它不仅给出判断，还生成判断的推理过程（CoT），这使得奖励信号更鲁棒，能有效对抗奖励黑客。在 VLA 中，这对应于“车辆是否在停车线内停稳”、“机械臂是否将物体插入指定孔位”。</li>
<li><strong>不可验证问题 (Non-verifiable Problems)</strong>: 对于那些依赖人类偏好的任务（如“以更‘拟人’的方式驾驶”、“生成一段对场景的‘生动’描述”，我们则依赖于上面提到的 RFT/RLHF 框架，即训练一个奖励模型来拟合人类的偏好。</li>
</ul>
</li>
</ul>
<h3 id="825-ope">8.2.5 OPE（离线策略评估）：不上线如何知好坏</h3>
<p>在将新训练的策略 $\pi_e$ 部署到昂贵或危险的真实环境中之前，我们迫切需要一种方法，利用已有的离线数据集（由旧的或行为策略 $\pi_b$ 收集）来评估 $\pi_e$ 的性能。</p>
<ul>
<li><strong>重要性采样 (Inverse Propensity Scoring, IPS)</strong>: 通过重要性权重 $\frac{\pi_e(a|s)}{\pi_b(a|s)}$ 来修正回报，但方差极大。</li>
<li><strong>双重鲁棒估计 (Doubly Robust, DR)</strong>: 结合了基于模型的估计（如 Q 函数）和 IPS，显著降低了方差，是当前最实用的 OPE 方法之一。</li>
<li><strong>拟合Q评估 (Fitted Q-Evaluation, FQE)</strong>: 直接在离线数据集上，为待评估的策略 $\pi_e$ 学习一个 Q 函数 $Q^{\pi_e}$。</li>
</ul>
<blockquote>
<p><strong>Rule-of-Thumb</strong>:
在进行任何 RL 微调后，<strong>必须</strong>运行 OPE。首选 DR 估计器来获得策略价值的点估计和置信区间。<strong>永远不要完全信任 OPE 的结果</strong>，它只是部署决策的辅助依据，而非最终判决。</p>
</blockquote>
<hr />
<h2 id="83">8.3 本章小结</h2>
<p>本章系统地阐述了如何将预训练 VLA 模型通过微调和强化学习，从一个模仿者提升为高效的决策者。我们明确了 SFT、RFT 和 RL 的核心差异与适用场景，并强调了它们在实践中通常是互补而非互斥的。</p>
<ul>
<li><strong>关键概念</strong>:<ul>
<li><strong>微调谱系</strong>: 从 SFT 到 RFT 再到 RL，是一个从纯粹模仿到价值对齐，再到自主优化的过程。</li>
<li><strong>稳定化RL技术</strong>: 对于需要长程推理的 VLA 任务，必须采用一套组合拳来稳定 RL 训练，包括<strong>价值预训练、解耦/自适应 GAE、非对称 PPO 裁剪</strong>和<strong>混合损失函数</strong>。</li>
<li><strong>奖励工程</strong>: 核心是区分<strong>可验证</strong>（使用自动化/思考式验证器）和<strong>不可验证</strong>（使用偏好奖励模型）的任务，并设计相应的奖励机制。</li>
<li><strong>自反式指导</strong>: 利用 LLM 的能力进行 CoT 评估和我纠正，是实现模型持续自我提升的有前途的方向。</li>
<li><strong>离线评估 (OPE)</strong>: IPS、DR 和 FQE 是在部署前评估策略性能的必要工具，为决策提供数据支持。</li>
<li><strong>策略整形</strong>: 平滑、延迟补偿和安全裕度是确保学习策略在真实世界中可用、可靠的最后一道工序。</li>
</ul>
</li>
</ul>
<hr />
<h2 id="84-gotchas">8.4 常见陷阱与错误 (Gotchas)</h2>
<ol>
<li>
<p><strong>奖励黑客 (Reward Hacking)</strong></p>
<ul>
<li><strong>现象</strong>: 智能体找到了一个最大化奖励的“捷径”，但这个行为完全违背了设计者的初衷。</li>
<li><strong>调试技巧</strong>: 对于可验证任务，使用更强大的验证器（如“思考式验证器”）来减少漏洞。对于不可验证任务，定期更新奖励模型，并引入多样化的、对抗性的偏好数据。在训练过程中定期审查智能体的行为录像。</li>
</ul>
</li>
<li>
<p><strong>长程任务的RL训练崩溃</strong></p>
<ul>
<li><strong>现象</strong>: 在需要长 CoT 或多步行动序列的任务中，PPO 训练过程非常不稳定，奖励曲线剧烈震荡，至完全不收敛。</li>
<li><strong>调试技巧</strong>: 不要期望 vanilla PPO 能直接解决问题。系统性地实施本章 8.2.3 节中介绍的稳定化技术组合：从价值预训练开始，确保优势估计的基线是可靠的；然后调整 GAE 和 PPO 裁剪策略；最后加入正样本 LM 损失来防止语言能力退化。这是一个系统工程，而非单个超参调整。</li>
</ul>
</li>
<li>
<p><strong>对 OPE 结果的盲目乐观</strong></p>
<ul>
<li><strong>现象</strong>: OPE 报告显示新策略性能提升 20%，但上线后表现反而下降。</li>
<li><strong>调试技巧</strong>: 始终报告 OPE 估计的<strong>置信区间</strong>。检查行为策略 $\pi_b$ 和评估策略 $\pi_e$ 的重合度，如果差异过大，OPE 结果将不可信。将 OPE 视为一个“红绿灯系统”，而非精确的数值预测。</li>
</ul>
</li>
<li>
<p><strong>灾难性遗忘 (Catastrophic Forgetting)</strong></p>
<ul>
<li><strong>现象</strong>: 在为任务 B 微调一个精通任务 A 的模型后，模型在任务 A 上的性能大幅下降。</li>
<li><strong>调试技巧</strong>: 在微调的数据中混入一部分原始任务 A 的数据。使用行为克隆正则化或<strong>正样本 LM 损失</strong>，确保新策略不会离原始策略太远。</li>
</ul>
</li>
<li>
<p><strong>忽略策略输出的物理可实现性</strong></p>
<ul>
<li><strong>现象</strong>: RL 策略输出了一个理论上最优但在物理上无法执行的行动序列（例如，要求电机瞬间改变速度）。</li>
<li><strong>调试技巧</strong>: 在奖励函数中加入对行动的平滑性惩罚。将策略输出视为一个“高层目标”，由一个低级的、满足动力学约束的控制器（如 MPC 或轨迹生成器，见第4章）来执行。</li>
</ul>
</li>
</ol>
            </article>
            
            <nav class="page-nav"><a href="chapter7.html" class="nav-link prev">← 第7章 预训练：模态预训练与跨模态对齐</a><a href="chapter9.html" class="nav-link next">第9章 基于仿真的智能体级强化学习（单智能体） →</a></nav>
        </main>
    </div>
</body>
</html>