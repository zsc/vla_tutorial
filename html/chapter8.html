<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第8章 强化学习与微调：从指令遵循到策略优化</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Visual-Language Action Model: 预训练、MARL 和 Sim-to-Real</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章 导论与动机案例</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章 视觉模态：从像素到可行动的表征</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章 语言模态：符号推理、过程编排与系统调度</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章 行动模态：从信号处理到鲁棒控制</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章 模态对齐（Vision–Language–Action）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章 隐式 3D 时空结构的引入</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章 预训练：模态预训练与跨模态对齐</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章 强化学习与微调：从指令遵循到策略优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章 基于仿真的智能体级强化学习（单智能体）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章 多智能体博弈与协调：从均衡理论与 MARL 到工程落地</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章 神经化 Sim-to-Real：弥合仿真与现实的认知与动力学鸿沟</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章 课程小实验设计（Lab）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第13章 大作业设计（Final Project）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第14章 结语：从范式到实践的闭环</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <p>好的，遵照您的指示，我将把您提供的关于<strong>强化微调 (RFT)</strong> 的详细解释，无缝地融入到我们已经增强过的 <code>chapter8.md</code> 中。</p>
<p>这次的修改将重点放在 <code>8.2.1</code> 节，利用 RFT 的概念来<strong>深化和澄清</strong>基于反馈的微调（之前称为 RFT/RLHF）的工作原理、动机和适用场景。我将保留之前版本的所有核心内容，包括来自两篇论文的实证洞见，同时用更清晰的框架和更生动的 VLA 场景案例来重构论述。</p>
<hr />
<h1 id="8">第8章 强化学习与微调：从指令遵循到策略优化</h1>
<h2 id="81">8.1 开篇段落</h2>
<p>在前面的章节中，我们已经构建了一个强大的 VLA 预训练基座模型，它能够通过模仿学习（如行为克隆）对世界产生初步的理解和行动能力。然而，仅仅“模仿”是不够的。真实世界的任务要求智能体能够<strong>优化</strong>性能、适应未见过的场景、并从交互的后果中学习，而不仅仅是复制专家数据。本章将深入探讨如何通强化学习（RL）和相关微调技术，将我们的 VLA 模型从一个被动的“模仿者”转变为一个主动的“决策者”。</p>
<p>我们将系统地比较监督微调（SFT）、<strong>强化微调 (RFT, 一种基于偏好学习的方法)</strong> 和经典环境交互式强化学习的优劣与适用场景。我们将引用最新的实证研究，剖析 RL 究竟能在 VLA 的哪些泛化维度（视觉、语义、执行）上带来超越 SFT 的收益。在此基础上，我们将深入探讨策略优化的核心算法，并介绍<strong>自反式指导</strong>、<strong>可验证奖励</strong>等前沿技术。特别地，我们将借鉴 <strong>VLA-R1</strong> 和 <strong>Seed1.5-Thinking</strong> 等前沿工作中的经验，深入探讨在处理需要复杂推理（如长思维链 CoT）的任务时，如何设计数据、奖励模型和算法来克服 RL 训练的<strong>不稳定性</strong>。</p>
<p>本章的学习目标是，掌握一套将预训练模型转化为能够进行目标导向优化、同时兼顾安全与效率的高级策略的完整方法论，学会使用离线策略评估（OPE）在部署前科学地评估其潜在性能，为后续的仿真与 Sim-to-Real 迁移奠定坚实的策略基础。</p>
<hr />
<h2 id="82">8.2 文字论述</h2>
<h3 id="821-sft-vs-rft-vs-rl">8.2.1 SFT vs. RFT vs. RL：记忆、偏好与泛化的权衡</h3>
<p>从预训练基座模型出发，提升其特定任务能力的路径并非只有一条。选择哪条路径，取决于数据可用性、任务的性质以及对泛化能力的追求。</p>
<ul>
<li>
<p><strong>监督微调 (Supervised Fine-Tuning, SFT) - 学习“做什么”</strong></p>
<ul>
<li>
<p><strong>核心思想</strong>: SFT 本质上是<strong>行为克隆 (Behavior Cloning, BC)</strong> 的延续。它使用高质量的“状态-行动”或“指令-答案”对 <code>(s, a)</code> 数据集，通过最大化专家行为的对数似然来进行微调。它教模型<strong>模仿</strong>一个已知的“正确答案”。
    $$
L_{SFT}(\theta) = -\mathbb{E}_{(s, a) \sim \mathcal{D}_{expert}} [\log \pi_\theta(a|s)]
$$</p>
</li>
<li>
<p><strong>优点</strong>: 简单、稳定、对数据质量要求高但利用直接。是冷启一个策略的最佳起点。</p>
</li>
<li><strong>缺点</strong>:<ol>
<li><strong>分布偏移 (Distribution Shift)</strong>: 模型在执行中遇到微小未见过的状态，可能导致错误累积。</li>
<li><strong>次优天花板</strong>: 模型的性能上限被专家数据所限制，无法发现超越专家的更优策略。实证研究表明，SFT 的性能会随着数据量的增加而饱和。</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>强化微调 (Reinforcement Fine-Tuning, RFT) - 学习“怎么做”</strong></p>
<ul>
<li><strong>核心思想</strong>: RFT 是一种基于<strong>偏好学习 (Preference Learning)</strong> 的高级微调技术。它承认了为复杂任务（如“以优雅的方式抓取”、“以安全的风格驾驶”）设计一个精确的奖励函数极其困难。因此，它不教模型模仿“正确答案”，而是通过奖励一个<strong>行为偏好</strong>来引导模型。其核心区别在于：SFT 教模型<strong>“说什么/做什么 (What to say/do)”</strong>，而 RFT 教模型<strong>“怎么说/怎么做 (How to say/do)”</strong>。</li>
<li><strong>关键组件</strong>:<ol>
<li><strong>基础型 (Base Model)</strong>: 我们想要微调的 VLA 模型。</li>
<li><strong>偏好数据集 (Preference Dataset)</strong>: 这是训练“裁判”的数据。数据格式是<strong>“prompt - chosen_completion - rejected_completion”</strong> 的三元组。在 VLA 领域，这对应于“<strong>状态/指令 - 更优的行动轨迹 - 较差的行动轨迹</strong>”。</li>
<li><strong>奖励模型 (Reward Model, RM)</strong>: 一个专门训练的模型，任务是给任何一个“(状态, 行动序列)”对打分。它通过学习偏好数据集，力求给 <code>chosen</code> 的行为打分高于 <code>rejected</code> 的行为。</li>
</ol>
</li>
<li>
<p><strong>工作流程三部曲</strong>:</p>
<ol>
<li><strong>准备偏好数据</strong>: 针对你的目标（如更平滑、更安全），创建包含 <code>(state, chosen_action_sequence, rejected_action_sequence)</code> 的数据集。</li>
<li>
<p><strong>训练奖励模型</strong>: 使用偏好数据训练 RM。其损失函数通常基于 Bradley-Terry 模型：
    $$
L(\phi) = -\mathbb{E}_{(s, a_w, a_l) \sim \mathcal{D}_{pref}} [\log(\sigma(r_\phi(s, a_w) - r_\phi(s, a_l)))]
$$
    其中 $a_w$ (winner) 是更优的行为，$a_l$ (loser) 是较差的行为。</p>
</li>
<li>
<p><strong>执行强化微调</strong>: 使用 PPO 等 RL 算法，以 RM 的输出作为奖励信号，微调基础模型。模型通过生成不同行为并从 RM 获得反馈，逐渐学会生成能获得高分的行为模式。</p>
<ul>
<li><strong>VLA 典型用例</strong>:</li>
<li><strong>提升动作的平滑性与效率</strong>:</li>
<li><code>prompt</code>: "将蓝色杯子放到水槽里"</li>
<li><code>chosen</code>: (一条平滑、直接的机械臂轨迹)</li>
<li><code>rejected</code>: (一条犹豫、多次停顿、有冗余移动的轨迹)</li>
<li><strong>减少不必要的行为拒绝（在安全边界内更乐于助人）</strong>:</li>
<li><code>prompt</code>: "在一个略显狭窄的空间中抓取物体"</li>
<li><code>chosen</code>: (一个经过仔细规划、成功避开障碍的轨迹)</li>
<li><code>rejected</code>: (模型判断过于保守，直接输出“无法执行”或静止不动)</li>
<li><strong>提升驾驶风格的“拟人性”或“舒适度”</strong>:</li>
<li><code>prompt</code>: (前方车辆减速的驾驶场景)</li>
<li><code>chosen</code>: (一个平滑、渐进的刹车动作)</li>
<li><code>rejected</code>: (一个突兀、让乘客不适的急刹车动作)</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>环境交互强化学习 (RL from Environment Interaction) - 学习“为了什么做”</strong></p>
<ul>
<li><strong>核心思想</strong>: 如果说 RFT 是从一个“品味裁判”（奖励模型）那里学习，那么这里的 RL 就是直接从“现实世界的后果”中学习。智能体直接与环境交互，接收由环境定义的、客观的标量奖励（如任务是否成功、是否发生碰撞、耗时多少），并以此优化策略。</li>
<li><strong>优点与实证发现</strong>:<ol>
<li><strong>超越模仿</strong>: 能够发现超越人类的、全新的解决方案。</li>
<li><strong>增强泛化 (来自《What Can RL Bring to VLA Generalization?》)</strong>:<ul>
<li><strong>执行 (Execution) 泛化</strong>: 收益<strong>极其显著</strong>。RL 通过试错探索，学会了如何从败中恢复，这是 SFT 和 RFT 都难以直接教授的。</li>
<li><strong>语义 (Semantics) 泛化</strong>: 收益<strong>中等</strong>。</li>
<li><strong>视觉 (Vision) 泛化</strong>: <strong>与 SFT 持平</strong>。</li>
</ul>
</li>
</ol>
</li>
<li><strong>缺点</strong>: <strong>样本效率极低</strong>，奖励函数设计是关键难点，且在线探索可能带来安全风险。</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Rule-of-Thumb</strong>:</p>
<ol>
<li><strong>起点 (SFT)</strong>: 永远从 SFT/BC 开始，教会模型任务的<strong>基本知识</strong>（“做什么”）。</li>
<li><strong>塑造品味 (RFT)</strong>: 当你需要调整模型的<strong>主观行为特性</strong>（如风格、语气、安全偏好，“怎么做”）时，使用 RFT。</li>
<li><strong>优化结果 (RL)</strong>: 当你有明确的、可量化的<strong>客观性能指标</strong>和高保真仿真器时，使用环境交互式 RL 来压榨性能极限（“为了什么做”）。</li>
<li><strong>混合使用</strong>: 最佳实践是三者结合：用 SFT 初始化，用 RFT 对齐主观偏好，同时用环境稀疏奖励作为补充。</li>
</ol>
</blockquote>
<h3 id="822">8.2.2 策略优化：核心算法与高效实践</h3>
<p>(本节内容与上一保持一致，主要介绍 PPO、离线 RL 以及高效 PPO 的实践配方，这些是实现 RFT 和环境交互 RL 的底层算法。)</p>
<h3 id="823-rl-seed15-thinking">8.2.3 稳定化RL：来自 Seed1.5-Thinking 的前沿实践</h3>
<p>(本节内容与上一版保持一致，介绍价值预训练、精细化优势估计、改进 PPO 裁剪和混合损失等高级稳定化技术。)</p>
<h3 id="824-rft">8.2.4 自反式指导与可验证奖励：RFT 的客观对应物</h3>
<p><strong>RFT 通过学习人类偏好来处理主观、难以量化的目标。然而，在 VLA 领域，许多任务的“好坏”是可以被客观、自动地验证的。</strong> 这就引出了与 RFT 互补的另一条强大路径：基于可验证奖励的 RL。</p>
<ul>
<li>
<p><strong>自反式指导与 CoT</strong>: <strong>VLA-R1</strong> 的工作完美诠释了这一点。</p>
<ol>
<li><strong>SFT 注入思考模式</strong>: 首先，在包含显式 <code>&lt;think&gt;...&lt;/think&gt;</code> 推理链的 CoT 数据集上进行 SFT。</li>
<li><strong>RL 优化思考与行动</strong>: 在 RL 阶段，模型生成的完整文本（思考+行动）将一并接受<strong>可验证奖励</strong>评估。</li>
</ol>
</li>
<li>
<p><strong>设计可验证奖励 (Verifiable Rewards)</strong>: <strong>VLA-R1</strong> 提出了一套精细化的、可自动验证的奖励函数：</p>
<ul>
<li><strong>轨迹奖励</strong>: 采用<strong>角度-长度增强 Fréchet 距离 (ALAF)</strong> 来奖励路径的平滑度和动态一致性。</li>
<li><strong>空间定位奖励</strong>: 使用<strong>广义交并比 (GIoU)</strong> 代替标准 IoU，在不重叠时也能提供有意义的梯度。</li>
<li><strong>格式奖励</strong>: 一个二元奖励，确保模型的输出严格遵守预定结构。</li>
<li><strong>消融研究证明</strong>: VLA-R1 的消融实验清晰地表明，<code>SFT &lt; SFT + CoT &lt; SFT + CoT + RL</code>。这证明了<strong>显式推理 (CoT)</strong> 和<strong>后续优化 (RL)</strong> 两者相辅相成，缺一不可。</li>
</ul>
</li>
</ul>
<p><strong>这揭示了一个核心的设计模式：当目标是主观的，使用 RFT；当目标是客观的，设计可验证奖励。</strong></p>
<h3 id="825-ope">8.2.5 OPE（离线策略评估）：不上线如何知好坏</h3>
<p>在将新训练的策略 $\pi_e$ 部署到昂贵或危险的真实环境中之前，我们迫切需要一种方法，利用已有的离线数据集（由旧的或行为策略 $\pi_b$ 收集）来评估 $\pi_e$ 的性能。</p>
<ul>
<li><strong>重要性采样 (Inverse Propensity Scoring, IPS)</strong>: 通过重要性权重 $\frac{\pi_e(a|s)}{\pi_b(a|s)}$ 来修正回报，但方差极大。</li>
<li><strong>双重鲁棒估计 (Doubly Robust, DR)</strong>: 结合了基于模型的估计（如 Q 函数）和 IPS，显著降低了方差，是当前最实用的 OPE 方法之一。</li>
<li><strong>拟合Q评估 (Fitted Q-Evaluation, FQE)</strong>: 直接在离线数据集上，为待评估的策略 $\pi_e$ 学习一个 Q 函数 $Q^{\pi_e}$。</li>
</ul>
<blockquote>
<p><strong>Rule-of-Thumb</strong>:
在进行任何 RL 微调后，<strong>必须</strong>运行 OPE。首选 DR 估计器来获得策略价值的点估计和置信区间。<strong>永远不要完全信任 OPE 的结果</strong>，它只是部署决策的辅助依据，而非最终判决。</p>
</blockquote>
<hr />
<h2 id="83">8.3 本章小结</h2>
<p>本章系统地阐述了如何将预训练 VLA 模型通过微调和强化学习，从一个模仿者提升为高效的决策者。我们明确了 SFT、RFT 和环境交互式 RL 的核心差异与适用场景。</p>
<ul>
<li><strong>关键概念</strong>:<ul>
<li><strong>微调谱系</strong>: SFT 教会<strong>“做什么”</strong>，RFT 塑造<strong>“怎么做”</strong>，而环境交互式 RL 优化<strong>“为了什么做”</strong>。</li>
<li><strong>偏好学习 (RFT)</strong>: 通过<strong>奖励模型</strong>和<strong>对比数据</strong>，将主观的人类偏好转化为可优化的学习信号，是塑造模型行为风格和价值观的关键。</li>
<li><strong>RL的非对称收益</strong>: RL 对 VLA 的泛化能力提升主要体现在<strong>执行鲁棒性</strong>上。</li>
<li><strong>可验证奖励</strong>: 与 RFT 处理主观目标相对应，为客观、可量化的任务维度设计精确的奖励函数是提升 RL 效率的另一条关键路径。</li>
<li><strong>稳定化与高效RL</strong>: 采用共享主干、预热、最小化 Epochs 等高效实践，并结合价值预训练、改进 GAE 等高级技术来稳定训练过程。</li>
<li><strong>离线评估 (OPE)</strong>: 在部署前评估策略性能的必要工具，为决策提供数据支持。</li>
</ul>
</li>
</ul>
<hr />
<h2 id="84-gotchas">8.4 常见陷阱与错误 (Gotchas)</h2>
<ol>
<li>
<p><strong>奖励模型被利用 (Reward Model Exploitation)</strong></p>
<ul>
<li><strong>现象</strong>: 这是 RFT 特有的风险。策略找到了一个方法来“欺骗”奖励模型获得高分，但产生的行为在人类看来却是无意义甚至错误的。这通常因为偏好数据覆盖的场景不够全面。</li>
<li><strong>调试技巧</strong>: 采用迭代式的数据收集和模型训练。定期用当前最优策略生成样本，让人类标注者找出其中“高分但低质”的例子，加入到偏好数据集中，然后重新训练奖励模型。</li>
</ul>
</li>
<li>
<p><strong>奖励黑客 (Reward Hacking)</strong></p>
<ul>
<li><strong>现象</strong>: 在环境交互式 RL 中，智能体找到了一个最大化奖励的“捷径”，但这个行为完全违背了设计者的初衷。</li>
<li><strong>调试技巧</strong>: 使用多维度的、结构化的可验证奖励来减少漏洞。在训练过程中定期审查智能体的行为录像。</li>
</ul>
</li>
<li>
<p><strong>长程任务的RL训练崩溃</strong></p>
<ul>
<li><strong>现象</strong>: 在需要长 CoT 或多步行动序列的任务中，PPO 训练过程非常不稳定。</li>
<li><strong>调试技巧</strong>: 系统性实施本章介绍的稳定化技术组合：从价值预训练开始，调整 GAE 和 PPO 裁剪策略，并加入正样本 LM 损失来防止语言能力退化。</li>
</ul>
</li>
<li>
<p><strong>对 OPE 结果的盲目乐观</strong></p>
<ul>
<li><strong>现象</strong>: OPE 报告性能提升，但上线后表现反而下降。</li>
<li><strong>调试技巧</strong>: 始终报告 OPE 估计的<strong>置信区间</strong>。检查行为策略和评估策略的重合度。</li>
</ul>
</li>
<li>
<p><strong>灾难性遗忘 (Catastrophic Forgetting)</strong></p>
<ul>
<li><strong>现象</strong>: 微调后，模型在原始任务上的性能大幅下降。</li>
<li><strong>调试技巧</strong>: 在微调数据中混入原始任务数据，并使用行为克隆正则化。</li>
</ul>
</li>
<li>
<p><strong>忽略策略输出的物理可实现性</strong></p>
<ul>
<li><strong>现象</strong>: RL 策略输出了物理上无法执行的行动序列。</li>
<li><strong>调试技巧</strong>: 在奖励函数中加入对行动的平滑性惩罚，并将策略输出视为高层目标，由低级控制器执行。</li>
</ul>
</li>
</ol>
            </article>
            
            <nav class="page-nav"><a href="chapter7.html" class="nav-link prev">← 第7章 预训练：模态预训练与跨模态对齐</a><a href="chapter9.html" class="nav-link next">第9章 基于仿真的智能体级强化学习（单智能体） →</a></nav>
        </main>
    </div>
</body>
</html>