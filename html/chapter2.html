<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第2章 视觉模态：从像素到可行动的表征</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Visual-Language Action Model: 预训练、MARL 和 Sim-to-Real</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章 导论与动机案例</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章 视觉模态：从像素到可行动的表征</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章 语言模态：符号推理、过程编排与系统调度</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章 行动模态</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章 模态对齐（Vision–Language–Action）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章 隐式 3D 时空结构的引入</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章 预训练：模态预训练与跨模态对齐</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章 强化学习与微调（模型级）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章 基于仿真的智能体级强化学习（单智能体）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章 多智能体博弈与协调：从均衡理论与 MARL 到工程落地</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章 神经化 Sim-to-Real：弥合仿真与现实的认知与动力学鸿沟</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="2">第2章 视觉模态：从像素到可行动的表征</h1>
<h2 id="_1">开篇段落</h2>
<p>视觉是具身智能体（Embodied Agent）感知和理解物理世界最主要、信息带宽最宽的通道。对于任何旨在“做对”而非仅仅“看懂”的VLA系统而言，视觉感知的失败是所有后续错误的根源，其代价可能是物理世界的实损失。本章的目标，正是为VLA模型构建一个坚实、可靠且“知其边界”的视觉感知基座。我们将沿着三条相互交织的技术主线，深入剖析视觉表征的演进：从奠定基石的卷积神经网络（CNN）及其分层、分部件的经典思想，到利用互联网规模图文数据进行大规模对比学习、打通视觉与语言鸿沟的对齐范式（如CLIP），再到挖掘视频数据内蕴时空结构与物理规律的自监督学习方法（如MAE）。本章不仅回顾这些方法的原理，更将系统性地梳理现代VLM如何基于这些思想构建出不同的主流架构。最终，所有讨论将收敛于两个对于构建可部署行动系统至关重要的议题：<strong>开集识别与不确定性估计</strong>，以及<strong>时域错位鲁棒性</strong>。这确保我们的感知基座不仅看得准，更能在面对未知时“知其不知”，并能优雅地处理真实世界中无处不在的延迟与异步。</p>
<hr />
<h2 id="_2">文字论述</h2>
<h3 id="21-cnn">2.1 经典视回顾：CNN 分层特征与“部件—整体”分解</h3>
<p>在深度学习革命之前，计算机视觉依赖于手工设计的特征提取器（如SIFT, HOG）。卷积神经网络（CNN）的崛起，标志着从“特征工程”到“特征学习”的范式转移。其成功源于对自然图像统计特性的深刻洞察和三个核心设计原则：</p>
<ol>
<li><strong>局部感受野 (Local Receptive Fields)</strong>：每个神经元只连接到输入层的一个小区域，这模拟了生物视觉皮层的工作方式，并利用了图像中像素相关性的局部性原理。</li>
<li><strong>权值共享 (Weight Sharing)</strong>：一个特征检测器（卷积核）在整个图像上滑动，共享同一套权重。这极大地减少了模型参数，并使其具备了<strong>平移不变性 (Translation Invariance)</strong>。</li>
<li><strong>层次化特征提取 (Hierarchical Feature Extraction)</strong>：通过堆叠卷积层和池化层，网络能够构建出从简单到复杂的特征层次。底层学习边缘、角点等基础纹理；中层组合物体的部件（如车轮、眼睛）；高层则汇聚成完整的物体或场景概念。</li>
</ol>
<ul>
<li><strong>代表性工作：ResNet (Residual Network, He et al., 2015)</strong><ul>
<li><strong>贡献</strong>：通过引入“残差连接”（Shortcut Connection），革命性地解决了深度网络训练的难题，使得训练数百甚至上千层的超深网络成为现实，并成为后续无数视觉模型的基础骨干。</li>
<li><strong>核心思想</strong>：与其让一个网络层（或块）直接学习一个复杂的目标映射 $H(x)$，不如让它学习该映射与输入 $x$ 之间的残差 $F(x) = H(x) - x$。原始映射则通过一个恒等短路连接变为 $H(x) = F(x) + x$。
$$
y = \mathcal{F}(x, \{W_i\}) + x
$$
这个简单的加法操作，为梯度在网络中的反向传播提供了一条“高速公路”，确保了信息的无损流动。</li>
</ul>
</li>
</ul>
<h3 id="22-marr">2.2 Marr 式表征思想与多尺度处理</h3>
<p>认知科学家大卫·马尔（David Marr）提出的视觉计算理论框架强调从多尺度信息中逐步恢复世界结构的重要性。这个思想在现代深度学习中，具体化为对<strong>多尺度特征</strong>的精妙处理。</p>
<ul>
<li><strong>代表性工作：FPN (Feature Pyramid Network, Lin et al., 2017)</strong><ul>
<li><strong>贡献</strong>：提出了一种简洁而高效的架构，用于在所有尺度上构建具有丰富语义信息的特征金字塔，极大地提升了目标检测和分割等任务的性能，已成为事实上的标准组件。</li>
<li><strong>结构</strong>：FPN通过自顶向下的路径和横向连接，将高层（语义丰富但分辨率低）特征图的信息逐步融合到低层（细节精确但语义弱）特征图中。</li>
</ul>
</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="w">      </span><span class="n">C5</span><span class="w"> </span><span class="p">(</span><span class="n">高语义</span><span class="p">)</span><span class="w"> </span><span class="o">---&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="n">x1</span><span class="w"> </span><span class="n">Conv</span><span class="p">]</span><span class="w"> </span><span class="o">-------------------&gt;</span><span class="w"> </span><span class="n">P5</span><span class="w"> </span><span class="p">(</span><span class="n">融合后</span><span class="p">)</span>
<span class="w">         </span><span class="o">|</span><span class="w">               </span><span class="o">|</span><span class="w">                              </span><span class="o">|</span>
<span class="w">         </span><span class="n">v</span><span class="w"> </span><span class="p">(</span><span class="n">自底向上</span><span class="p">)</span><span class="w">      </span><span class="o">+--&gt;</span><span class="w"> </span><span class="p">[</span><span class="n">Upsample</span><span class="w"> </span><span class="n">x2</span><span class="p">]</span><span class="w"> </span><span class="o">--</span><span class="p">(</span><span class="o">+</span><span class="p">)</span><span class="o">--&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">3</span><span class="n">x3</span><span class="w"> </span><span class="n">Conv</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">P4</span>
<span class="w">         </span><span class="o">|</span><span class="w">               </span><span class="o">|</span><span class="w">                     </span><span class="o">^</span>
<span class="w">      </span><span class="n">C4</span><span class="w"> </span><span class="o">------------&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="n">x1</span><span class="w"> </span><span class="n">Conv</span><span class="p">]</span><span class="w"> </span><span class="o">--------------+</span>
<span class="w">      </span><span class="p">...</span><span class="w">             </span><span class="p">...</span><span class="w">                     </span><span class="p">...</span>
</code></pre></div>

<p><strong>经验法则 (Rule-of-thumb):</strong> 在处理需要同时关注全局与局部感知的机器人或自动驾驶任务时，使用FPN或其变体结构（如PANet, BiFPN）几乎是必须的。</p>
<h3 id="23-vlm">2.3 视觉—语言对齐与现代VLM架构范式</h3>
<p>CLIP的成功不仅验证了大规模图文对比学习的有效性，更催生了现代VLM的主流架构范式。核心问题变为：如何最高效、最有力地将强大的预训练视觉编码器（如ViT）与预训练语言模型（LLMs）结合起来？不同的回答形成了当今VLM世界的三大主流设计哲学。</p>
<ul>
<li>
<p><strong>奠基性工作：CLIP (Radford et al., 2021) 与 SigLIP/SigLIP 2 (Zhai et al., 2023; Tschannen et al., 2025)</strong></p>
<ul>
<li><strong>CLIP</strong>：通过<strong>对比学习（Contrastive Learning）</strong>目标，在海量图文对上训练独立的图像和文本编码器。其InfoNCE损失函数的目标是在一个batch内，将匹配的（图，文）对的特征相似度推高，同时将其余所有不匹配的对的相似度压低。这本质上是一个“一对多”的分类问题。</li>
<li><strong>SigLIP (Sigmoid Loss for Language-Image Pre-training)</strong>：是对CLIP训练范式的一次重要优化。它指出，CLIP的全局对比损失函数在超大batch下效率不高。SigLIP将其简化为一个更直接的<strong>sigmoid二元分类</strong>目标：对于任意一个（图，文）对，模型只需判断它们是否匹配。这种“一对一”的判断，使得训练可以在更小的batch下进行，且实验证明其扩展性更好，在同等计算量下通常能获得更优的性能。</li>
<li><strong>SigLIP 2</strong>：进一步将多种先进技术统一到一个训练配方中，极大地增强了视觉编码器的能力，尤其是<strong>局部化（Localization）和密集特征（Dense Features）</strong>的能力。其核心贡献包括：<ol>
<li><strong>融合Decoder-based损失</strong>：在SigLIP的sigmoid损失之外，额外增加了一个轻量级decoder（类似PaLI/LocCa），用于执行captioning和referring expression等任务。这迫使视觉编码器不仅要学习全局语义，还要学习与文本短语对应的<strong>局部区域</strong>特征。</li>
<li><strong>引入自监督损失</strong>：在训练的后期阶段，加入类似DINO的<strong>自蒸馏（self-distillation）</strong>和<strong>掩码预测（masked prediction）</strong>损失。这鼓励模型学习到更鲁棒、更稠密的像素级特征，对于分割、深度估计等密集预测任务至关重要。</li>
<li><strong>多语言与数据去偏</strong>：训练数据中包含了更丰富的多语言文本，并应用了去偏技术，使得模型在跨文化理解和公平性方面表现更佳。</li>
</ol>
</li>
</ul>
<p>SigLIP/SigLIP 2的演进，代表了视觉编码器从单纯追求“全局语义对齐”向“全局与局部、稀疏与稠密特征兼顾”的转变，为下游VLM提供了能力更全面的视觉“脊梁”。</p>
</li>
</ul>
<h4 id="231-flamingo">2.3.1 范式一：面向巨人的适配器——以Flamingo为代表</h4>
<p>这是“站在巨人肩膀上”的典范，其核心思想是<strong>最大化地复用并保持强大的预训练视觉编码器和LLM基本冻结</strong>，仅通过一个轻级的、可训练的“适配器”模块来高效地桥接两者，以弥合模态鸿沟（modality gap）。</p>
<ul>
<li><strong>代表性工作：Flamingo (Alayrac et al., 2022)</strong><ul>
<li><strong>设计哲学</strong>：在计算资源有限或希望快速验证想法时，完全从零训练或全量微调巨大的基座模型是不切实际的。Flamingo假设视觉和语言的通用知识已分别被其基座模型充分学习，唯一需要学习的是如何将视觉概念“翻译”成LLM能够理解的“语言”。</li>
<li><strong>架构创新</strong>：<ol>
<li><strong>Perceiver Resampler</strong>：这是一个精巧的瓶颈模块。视觉编码器（如ViT）对图像会产生上百个特征向量（tokens），而将这么多向量直接注入LLM会带来巨大的计算负担。Perceiver Resampler通过少量可学习的“查询”向量，利用交叉注意力机制将可变数量的视觉特征“蒸馏”成固定数量（例如64个）的、低维的“潜”向量。这本质上是一种可学习的视觉信息压缩。</li>
<li><strong>Gated Cross-Attention Layers</strong>：这些新引入的、可训练的交叉注意力层被<strong>交错地（interleaved）</strong>插入到<strong>冻结的</strong>LLM的原始自注意力层之间。在每个这样的层，语言token可以attend到由Perceiver Resampler生成的视觉潜向量。关键在于“Gated”（门控）机制，它通过一个学习到的标量来控制该层交叉注意力的输出应在多大程度上影响LLM的中间表示，让模型自己决定在推理的不同阶段“看多少眼”图像。</li>
</ol>
</li>
<li><strong>优势与影响</strong>：由于只训练极少量的参数（适配器部分），Flamingo的训练效率极高。它展示了惊人的少样本（few-shot）上下文学习能力，能够通过几个例子快速适应新的图文任务，为参数高效的VLM设计树立了标杆。</li>
</ul>
</li>
</ul>
<h4 id="232-palipaligemmainternvl">2.3.2 范式二：端到端与多阶段联合训练——以PaLI/PaliGemma和InternVL为代表</h4>
<p>与冻结组件不同，该范式主张对视觉和语言模块进行更深度的联合训练，以期获得更强性能上限。其哲学是，模态间的深度融合需要让两个模型的参数都能相互适应，而不仅仅是单向的“翻译”。这通常采用复杂而精心设计的多阶段训练策略。</p>
<ul>
<li><strong>代表性工作：PaLI/PaliGemma/PaliGemma2 (Chen et al., 2022; Beyer et al., 2024)</strong><ul>
<li><strong>设计哲学</strong>：模型性能的极限最终取决于数据和模型参数的协同优化。通过精心设计的训练日程，可以在不同阶段侧重于学习不同层面的能力（通用语义、空间关系、高分辨率细节等），从而在最终的下游任务上获得最佳的迁移性能。</li>
<li><strong>训练策略</strong>：<code>PaliGemma</code>论文中描述了一个典型的多阶段流程，这已成为开源社区的主流实践：<ul>
<li><strong>Stage 0 (单模态预训练)</strong>：直接复用在各自领域（如<code>PaliGemma</code>使用<strong>SigLIP</strong>和<strong>Gemma</strong>）预训练好的视觉和语言模型作为初始化。</li>
<li><strong>Stage 1 (多模态预训练)</strong>：在一个由图像描述、视觉问答、OCR、物体检测等任务<strong>精心混合</strong>的大规模图文数据上联合训练整个模型。与Flamingo的关键不同在于，<strong>此阶段通常会解冻视觉编码器</strong>。其动机在于，CLIP/SigLIP这类对比学习模型擅长全局语义，但在需要精确定位的任务上可能并非最优。允许视觉编码器在更丰富的监督信号下微调，可以使其学习到对下游任务更有价值的空间关系理解能力。</li>
<li><strong>Stage 2 (分辨率提升)</strong>：在更高分辨率的图像上进行短时间的持续预训练。</li>
<li><strong>Stage 3 (迁移微调)</strong>：在具体的下游任务上进行最终的微调。</li>
</ul>
</li>
</ul>
</li>
<li><strong>代表性工作：InternVL (Chen et al., 2023) 和 Qwen-VL (Bai et al., 2023)</strong>：这两个模型是该范式在“规模”上的极致体现。它们通过在海量（百亿甚至千亿级token）、高质量、经严格清洗和配比的多模态数据上进行持续的多阶段预训练，将开源VLM的性能推向了新的高度，展现了端到端联合训练在深度融合多模态信息上的巨大潜力。</li>
</ul>
<h4 id="233-ofaemu3">2.3.3 范式三：万物皆为序列的统一——以OFA和Emu3为代表</h4>
<p>这是最具野心和理论优雅性的范式，其目标是将<strong>所有模态（图像、视频、文本、语音等）都编码为统一的离散token序列</strong>，然后用一个单一的、巨大的自回归Transformer模型，通过标准的“下一个token预测”（Next-Token Prediction）目标来进行统一建模。</p>
<ul>
<li><strong>代表性工作：OFA (One-For-All, Wang et al., 2022)</strong><ul>
<li><strong>设计哲学</strong>：任务的边界是人为的。无论是图像描述（输入图，输出文）、视觉问答（输入图+文，输出文）还是物体检测（输入图+文，输出坐标），都可以统一为一种“序列到序列”的生成任务。</li>
<li><strong>架构创新</strong>：OFA通过一个共享的词表来表示不同信息：文本用BPE分词，图像区域通过将其坐标（x, y, w, h）<strong>量化</strong>成离散的token来表示，图像内容则通过一个预训练的VQ-VAE编码器压缩成离散的视觉token。然后，一个标准的Transformer编码器-解码器模型被训练来处理这些混合了不同类型token的序列。</li>
</ul>
</li>
<li><strong>代表性工作：Emu/Emu2/Emu3 (BAAI, 2023/2024)</strong><ul>
<li><strong>设计哲学</strong>：“Next-Token Prediction is All You Need”。如果说OFA还依赖于预训练的VQ-VAE，Emu3则将这一理念推向极致。它致力于构建一个真正端到端的、仅依赖单一目标的系统。</li>
<li><strong>架构创新</strong>：Emu3的核心是一个强大的<strong>视觉分词器（Vision Tokenizer）</strong>。这是一个独立的、经过精心训练的编码器-解码器模型，其唯一任务就是将图像甚至视频帧无损地“压缩”成离散的token序列，并能从这些token中完美重建回原始像素。一旦有了这个分词器，整个VLM的训练就变得异常简单：将文本token序列和视觉token序列拼接起来，然后用一个巨大的、从零开始训练的纯解码器Transformer模型（类似GPT）去预测下一个token。</li>
<li><strong>优势与影响</strong>：架构极其简洁统一，摆脱了对CLIP、扩散模型等其他预训练组件的依赖。它天然地统一了<strong>感知</strong>（通过编码序列来理解）和<strong>生成</strong>（通过自回归地预测视觉token来创造图像和视频），展示了用语言模型的基本原理统一多模态AI的巨大潜力。</li>
</ul>
</li>
</ul>
<h3 id="24">2.4 超越固定尺寸：处理高分辨率与可变尺寸输入</h3>
<p>早期VLM的一个普遍限制是它们只能处理固定尺寸、通常是正方形的低分辨率输入（如224x224），这对于需要精细细节的任务（如OCR、细粒度识别）是致命的。近期工作开始致力于打破这一限制，这对具身智能尤为重要。</p>
<ul>
<li><strong>代表性工作：Kimi-VL (Moonshot AI, 2025) 与 Seed-1.5-VL (ByteDance, 2025)</strong><ul>
<li><strong>核心思想</strong>：采用<strong>原生分辨率视觉编码器 (Native-resolution Vision Encoder)</strong>。其灵感源于NaViT，核心在于放弃将不同尺寸的图像强制缩放到固定大小。取而代之的是，无论图像原始尺寸和长宽比如何，都将其统一划分为固定大小的patch（例如14x14）。</li>
<li><strong>技术实现</strong>：<ol>
<li><strong>打包（Packing）</strong>：不同图像会产生数量不同的patch。这些patch序列被“打包”成一个更长的、连续的1D序列输入到Transformer中，不同图像的patch之间用特殊分隔符隔开，并通过注意力掩码（attention mask）确保它们在计算中相互隔离。</li>
<li><strong>2D RoPE（旋转位置编码）</strong>：由于patch序列的长度可变，传统的学习式1D位置编码不再适用。2D RoPE被用来对每个patch在原始2D图像中的<code>（x, y）</code>坐标进行编码，赋予模型灵活处理任意尺寸和长宽比输入的能力。</li>
</ol>
</li>
</ul>
</li>
<li><strong>代表性工作：MiniMax-VL (MiniMax, 2025)</strong><ul>
<li><strong>核心思想</strong>：采用动态多尺度策略。它不依赖于单一的原生分辨率输入，而是将输入图像处理成多个不同分辨率的版本，包括一个标准的低分辨率缩略图和多个高分辨率的裁剪块。</li>
<li><strong>技术实现</strong>：这些多尺度的图像块被独立编码后，其特征在送入LLM之前被融合。这种策略让模型能同时获得“全局概览”（来自低分辨率缩略图）和“局部细节”（来自高分辨率块）。同时，其对<code>Lightning Attention</code>（一种线性注意力变体）的探索也旨在高效处理高分辨率图像产生的巨量token序列。</li>
</ul>
</li>
</ul>
<h3 id="25">2.5 视频自/半监督、开放问题与路线组合</h3>
<p>视频自监督学习（如<strong>VideoMAE</strong>）通过掩码重建等任务，让模型从无标签视频中学习时空结构。然而，视觉模态的<strong>符号接地鸿沟</strong>和<strong>压缩 vs. 抽象</strong>的争议仍是开放问题。
<strong>经验法则 (Rule-of-thumb):</strong> 一个稳健的视觉基座构建策略通常遵循阶段化设计：</p>
<ol>
<li><strong>阶段一（获取通用语义先验）</strong>：使用在海量图文对上预训练好的模型（如SigLIP的ViT变体）作为视觉编码器的<strong>初始化权重</strong>。</li>
<li><strong>阶段二（学习领域动态特性）</strong>：在量领域相关视频上，使用掩码重建（如VideoMAE）进行第二阶段预训练。</li>
<li><strong>阶段三（任务导向的微调）</strong>：在最终目标任务上进行端到端微调。</li>
</ol>
<h3 id="26">2.6 开集识别、不确定性估计与时域错位鲁棒性</h3>
<p>对于需要做出物理行动的VLA系统，处理未知和不确定性是保证安全的核心要求。</p>
<ol>
<li>
<p><strong>开集识别与不确定性估计 (Open-Set Recognition &amp; Uncertainty Estimation)</strong></p>
<ul>
<li><strong>问题</strong>：模型在部署时会遇到分布外（OOD）的物体或场景。一个安全的系统不应强行分类，而应报告“未知”或低置信度。</li>
<li><strong>方法：能量分数 (Energy Score)</strong>：一个理论优雅的方法。对于分类器的logits $f_i(x)$，其能量函数可定义为 $E(x; f) = - \log \sum_i e^{f_i(x)}$。分布内样本的能量分数通常远低于OOD样本，因此可以设置一个阈值来高效地检测异常输入。</li>
</ul>
</li>
<li>
<p><strong>时域错位鲁棒性 (Temporal Misalignment Robustness)</strong></p>
<ul>
<li><strong>问题</strong>：真实系统中传感器的数据传输、预处理和模型推理都需要时间，导致模型接收到的视觉信息总是“过去时”。</li>
<li><strong>缓解策略</strong>：<ul>
<li><strong>精确时间戳与系统级同步</strong>：这是基石。</li>
<li><strong>状态估计与预测</strong>：在感知和决策之间引入一个状态估计器（如卡尔曼滤波器），根据历史观测来预测“当前时刻”的真实世界状态。</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr />
<h2 id="_3">本章小结</h2>
<p>本章我们系统地剖析了构建一个现代、强大的VLA视觉基座所涉及的核心技术与思想演进。我们不仅回顾了经典视觉（CNN）、视觉-语言对齐（CLIP及其后续如SigLIP/SigLIP 2）和视频自监督（MAE）等奠基性思想，更深入地剖析了现代VLA模型的三大主流架构范式：</p>
<ol>
<li><strong>适配器范式 (如 Flamingo)</strong>：冻结强大的基座模型，通过轻量级模块进行高效桥接，擅长少样本学习，是参数高效的典范。</li>
<li><strong>联合训练范式 (如 PaliGemma, InternVL)</strong>：通过端到或多阶段的联合训练，实现模态间的深度融合，追求更高的性能上限，是当前开源社区的主流。</li>
<li><strong>统一序列范式 (如 OFA, Emu3)</strong>：将所有模态分词化，用单一的Next-Token Prediction目标统一所有任务，架构优雅，是通往通用多模态智能的潜力路径。</li>
</ol>
<p>我们还探讨了处理<strong>高分辨率和可变尺寸输入</strong>的前沿方法（如Kimi-VL, Seed-1.5-VL），这对于提升VLA在精细任务上的能力至关重要。最后，一个面向行动的理想视觉基座，必须能够量化自身对输入的不确定性（<strong>能量分数、集成方法</strong>），并能稳健地处理真实物理世界中无处不在的<strong>时域错位</strong>（<strong>时间戳、状态预测</strong>）。这些特性是后续章节中实现可靠对齐、安全RL和成功Sim-to-Real迁移的绝对前提。</p>
<hr />
<h2 id="gotchas">常见陷阱与错误 (Gotchas)</h2>
<ol>
<li>
<p><strong>陷阱：将 ImageNet 精度等同于真实世界鲁棒性。</strong></p>
<ul>
<li><strong>表现</strong>：过度迷信在标准学术数据集上的SOTA排名，而忽略了模型在特定部署环境下的长尾场景表现。</li>
<li><strong>调试技巧</strong>：建立并维护一个专门针对“痛点”的评估集（"Pain-Point" Benchmark），包含大量来自真实部署环境的失败案例、边界条件和对抗性样本。</li>
</ul>
</li>
<li>
<p><strong>陷阱：混淆“基础模型”与“指令微调模型”的评测目标。</strong></p>
<ul>
<li><strong>表现</strong>：使用一个为“迁移微调”（Transfer Fine-tuning）设计的“基础VLM”（如PaliGemma）去直接进行零样本的指令跟随评测，并得出其性能不佳的结论。</li>
<li><strong>调试技巧</strong>：明确模型的设计目标。基础模型（Base Model）的目标是学习通用、可迁移的表征，其评测应侧重于下游任务微调后的性能。指令微调模型（Instruction-tuned Model）的目标是遵循零样本或少样本指令，其评测应侧重于直接的问答和指令跟随能力。</li>
</ul>
</li>
<li>
<p><strong>陷阱：模型“自信地幻视”而未被系统拦截。</strong></p>
<ul>
<li><strong>表现</strong>：一个VLM描述图像时，生成了听起来非常合理但图像中完全不存在的物体或细节（即“幻觉”，Hallucination），且输出置信度很高。</li>
<li><strong>调试技巧</strong>：绝不单独依赖softmax输出作为置信度。部署流程中强制加入不确定性监控（如能量分数）。此外，可以引入“可追溯性”或“引用”机制，要求模型在生成描述时，指出其描述对应图像的哪个区域。</li>
</ul>
</li>
<li>
<p><strong>陷阱：忽视视觉输入的时域特性。</strong></p>
<ul>
<li><strong>表现</strong>：模型对时间抖动、延迟或丢帧极其敏感，因为其没有学到物体的运动连续性和动态一致性。</li>
<li><strong>调试技巧</strong>：在仿真和回放测试中，系统性地注入时间扰动。检查所有传感器的同步日志，确保时间戳对齐是硬件和软件层面的第一优先级。</li>
</ul>
</li>
</ol>
            </article>
            
            <nav class="page-nav"><a href="chapter1.html" class="nav-link prev">← 第1章 导论与动机案例</a><a href="chapter3.html" class="nav-link next">第3章 语言模态：符号推理、过程编排与系统调度 →</a></nav>
        </main>
    </div>
</body>
</html>